{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用非监督式学习进行数据流的异常检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非监督式学习在异常检测方面十分重要。因为攻击方式在不断更新，需要一个可以迭代的模型，能够对对未知入侵作出反应。本次实验是以Autoencoder为主体，以正常、安全的数据包为训练集，建立非监督学习模型，对不能够还原（编码、解码）的异常数据进行检查。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集为NSL-KDD，本次实验提取的是其中TCP协议下的数据包，带有53600条普通数据报文以及11038条攻击报文。数据集下载地址：https://www.unb.ca/cic/datasets/nsl.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys list on the position 2: ['ftp_data', 'http', 'telnet', 'smtp', 'finger', 'ftp', 'time', 'other', 'auth', 'shell', 'pop_3', 'IRC', 'domain', 'X11', 'private', 'imap4', 'ssh', 'link', 'remote_job', 'ldap', 'courier', 'discard', 'mtp', 'systat', 'iso_tsap', 'csnet_ns', 'uucp', 'whois', 'netbios_ns', 'Z39_50', 'sunrpc', 'netbios_dgm', 'uucp_path', 'vmnet', 'name', 'pop_2', 'http_443', 'login', 'gopher', 'exec', 'kshell', 'sql_net', 'hostnames', 'echo', 'daytime', 'pm_dump', 'netstat', 'ctf', 'nntp', 'netbios_ssn', 'supdup', 'bgp', 'nnsp', 'rje', 'printer', 'efs', 'klogin']\n",
      "The keys list on the position 3: ['SF', 'REJ', 'RSTO', 'S0', 'S1', 'RSTR', 'S3', 'S2', 'OTH', 'SH', 'RSTOS0']\n"
     ]
    }
   ],
   "source": [
    "%run PreProcessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的预处理：去掉冗余字段，将训练数据映射至0到1区间，使得训练可以更好收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，实验为了模拟未来可能出现的场景（模型在云端不断抓取正常数据进行更新，而入侵数据形式是未知的），对测试数据除以的是训练数据的最大值。也就是说，测试数据并不保证在0到1的区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：自动解码器Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 首先使用整个正常数据集进行训练，目的是验证实验可行性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83625\\DataIDPS\\Autoencoder\\NSL_KDD_AE_V2.py:69: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pack_train = pack_train / np.max(pack_train, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 42880 samples, validate on 10720 samples\n",
      "Epoch 1/5000\n",
      "42880/42880 [==============================] - 3s 80us/step - loss: 0.0360 - val_loss: 0.0061\n",
      "Epoch 2/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 3/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 4/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 5/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 6/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 7/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 9/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5135e-04 - val_loss: 9.1707e-04\n",
      "Epoch 10/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5249e-04 - val_loss: 8.1976e-04\n",
      "Epoch 11/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5661e-04 - val_loss: 7.3231e-04\n",
      "Epoch 12/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 6.7537e-04 - val_loss: 6.6355e-04\n",
      "Epoch 13/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 6.1571e-04 - val_loss: 6.1214e-04\n",
      "Epoch 14/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 5.6169e-04 - val_loss: 5.4413e-04\n",
      "Epoch 15/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 5.0590e-04 - val_loss: 4.8745e-04\n",
      "Epoch 16/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 4.6480e-04 - val_loss: 4.5370e-04\n",
      "Epoch 17/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 4.3298e-04 - val_loss: 4.3022e-04\n",
      "Epoch 18/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 4.0941e-04 - val_loss: 4.0083e-04\n",
      "Epoch 19/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.9355e-04 - val_loss: 3.9238e-04\n",
      "Epoch 20/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.8018e-04 - val_loss: 3.8673e-04\n",
      "Epoch 21/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.6930e-04 - val_loss: 3.7202e-04\n",
      "Epoch 22/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 3.5915e-04 - val_loss: 3.6929e-04\n",
      "Epoch 23/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.4976e-04 - val_loss: 3.5536e-04\n",
      "Epoch 24/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 3.4177e-04 - val_loss: 3.4668e-04\n",
      "Epoch 25/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 3.3505e-04 - val_loss: 3.4357e-04\n",
      "Epoch 26/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.2899e-04 - val_loss: 3.4035e-04\n",
      "Epoch 27/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.2250e-04 - val_loss: 3.3235e-04\n",
      "Epoch 28/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.1693e-04 - val_loss: 3.2353e-04\n",
      "Epoch 29/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 3.1163e-04 - val_loss: 3.2040e-04\n",
      "Epoch 30/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 3.0579e-04 - val_loss: 3.1409e-04\n",
      "Epoch 31/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 3.0102e-04 - val_loss: 3.1017e-04\n",
      "Epoch 32/5000\n",
      "42880/42880 [==============================] - 2s 41us/step - loss: 2.9774e-04 - val_loss: 3.0247e-04\n",
      "Epoch 33/5000\n",
      "42880/42880 [==============================] - 2s 50us/step - loss: 2.9319e-04 - val_loss: 3.0015e-04\n",
      "Epoch 34/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.8921e-04 - val_loss: 3.0436e-04\n",
      "Epoch 35/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.8491e-04 - val_loss: 2.9503e-04\n",
      "Epoch 36/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.8241e-04 - val_loss: 2.9661e-04\n",
      "Epoch 37/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 2.7921e-04 - val_loss: 2.9076e-04\n",
      "Epoch 38/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.7559e-04 - val_loss: 2.9294e-04\n",
      "Epoch 39/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.7299e-04 - val_loss: 2.8184e-04\n",
      "Epoch 40/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.7083e-04 - val_loss: 2.8069e-04\n",
      "Epoch 41/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.6842e-04 - val_loss: 2.7605e-04\n",
      "Epoch 42/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.6466e-04 - val_loss: 2.7527e-04\n",
      "Epoch 43/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.6198e-04 - val_loss: 2.6944e-04\n",
      "Epoch 44/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.5932e-04 - val_loss: 2.7150e-04\n",
      "Epoch 45/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 2.5574e-04 - val_loss: 2.7089e-04\n",
      "Epoch 46/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.5009e-04 - val_loss: 2.4576e-04\n",
      "Epoch 47/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 2.2860e-04 - val_loss: 2.3034e-04\n",
      "Epoch 48/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.1921e-04 - val_loss: 2.2271e-04\n",
      "Epoch 49/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 2.1366e-04 - val_loss: 2.2655e-04\n",
      "Epoch 50/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.1021e-04 - val_loss: 2.1901e-04\n",
      "Epoch 51/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.0588e-04 - val_loss: 2.1570e-04\n",
      "Epoch 52/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 2.0151e-04 - val_loss: 2.1023e-04\n",
      "Epoch 53/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 2.0002e-04 - val_loss: 2.1205e-04\n",
      "Epoch 54/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.9754e-04 - val_loss: 2.0396e-04\n",
      "Epoch 55/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.9495e-04 - val_loss: 2.0554e-04\n",
      "Epoch 56/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.9235e-04 - val_loss: 2.0440e-04\n",
      "Epoch 57/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.9012e-04 - val_loss: 2.0150e-04\n",
      "Epoch 58/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.8790e-04 - val_loss: 2.0237e-04\n",
      "Epoch 59/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.8667e-04 - val_loss: 1.9625e-04\n",
      "Epoch 60/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.8412e-04 - val_loss: 1.9398e-04\n",
      "Epoch 61/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.8328e-04 - val_loss: 1.9855e-04\n",
      "Epoch 62/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.8092e-04 - val_loss: 1.9202e-04\n",
      "Epoch 63/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.7882e-04 - val_loss: 1.9198e-04\n",
      "Epoch 64/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.7758e-04 - val_loss: 1.8918e-04\n",
      "Epoch 65/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.7742e-04 - val_loss: 1.8939e-04\n",
      "Epoch 66/5000\n",
      "42880/42880 [==============================] - 1s 35us/step - loss: 1.7522e-04 - val_loss: 1.9429e-04\n",
      "Epoch 67/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.7502e-04 - val_loss: 1.8720e-04\n",
      "Epoch 68/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.7260e-04 - val_loss: 1.8732e-04\n",
      "Epoch 69/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.7291e-04 - val_loss: 1.8405e-04\n",
      "Epoch 70/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.7076e-04 - val_loss: 1.8431e-04\n",
      "Epoch 71/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.6967e-04 - val_loss: 1.8241e-04\n",
      "Epoch 72/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.7081e-04 - val_loss: 1.8412e-04\n",
      "Epoch 73/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.6767e-04 - val_loss: 1.7855e-04\n",
      "Epoch 74/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6759e-04 - val_loss: 1.7859e-04\n",
      "Epoch 75/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6573e-04 - val_loss: 1.7805e-04\n",
      "Epoch 76/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6481e-04 - val_loss: 1.7948e-04\n",
      "Epoch 77/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6355e-04 - val_loss: 1.7531e-04\n",
      "Epoch 78/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6208e-04 - val_loss: 1.7501e-04\n",
      "Epoch 79/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.6113e-04 - val_loss: 1.7390e-04\n",
      "Epoch 80/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.6016e-04 - val_loss: 1.7844e-04\n",
      "Epoch 81/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.5966e-04 - val_loss: 1.7241e-04\n",
      "Epoch 82/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.5767e-04 - val_loss: 1.7367e-04\n",
      "Epoch 83/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.5627e-04 - val_loss: 1.6843e-04\n",
      "Epoch 84/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.5366e-04 - val_loss: 1.6775e-04\n",
      "Epoch 85/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.5267e-04 - val_loss: 1.6322e-04\n",
      "Epoch 86/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.5098e-04 - val_loss: 1.5924e-04\n",
      "Epoch 87/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.5067e-04 - val_loss: 1.6283e-04\n",
      "Epoch 88/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4983e-04 - val_loss: 1.6254e-04\n",
      "Epoch 89/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.4787e-04 - val_loss: 1.5804e-04\n",
      "Epoch 90/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4754e-04 - val_loss: 1.5441e-04\n",
      "Epoch 91/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.4681e-04 - val_loss: 1.5970e-04\n",
      "Epoch 92/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4553e-04 - val_loss: 1.5677e-04\n",
      "Epoch 93/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4610e-04 - val_loss: 1.5493e-04\n",
      "Epoch 94/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4469e-04 - val_loss: 1.5815e-04\n",
      "Epoch 95/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4452e-04 - val_loss: 1.5560e-04\n",
      "Epoch 96/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4308e-04 - val_loss: 1.5478e-04\n",
      "Epoch 97/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4221e-04 - val_loss: 1.5209e-04\n",
      "Epoch 98/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4107e-04 - val_loss: 1.5263e-04\n",
      "Epoch 99/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3977e-04 - val_loss: 1.5130e-04\n",
      "Epoch 100/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.3938e-04 - val_loss: 1.4613e-04\n",
      "Epoch 101/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.4035e-04 - val_loss: 1.4865e-04\n",
      "Epoch 102/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.3874e-04 - val_loss: 1.4400e-04\n",
      "Epoch 103/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3745e-04 - val_loss: 1.4720e-04\n",
      "Epoch 104/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.3676e-04 - val_loss: 1.5300e-04\n",
      "Epoch 105/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3760e-04 - val_loss: 1.4889e-04\n",
      "Epoch 106/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3627e-04 - val_loss: 1.4673e-04\n",
      "Epoch 107/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3572e-04 - val_loss: 1.4370e-04\n",
      "Epoch 108/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3517e-04 - val_loss: 1.4119e-04\n",
      "Epoch 109/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.3387e-04 - val_loss: 1.4101e-04\n",
      "Epoch 110/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.3393e-04 - val_loss: 1.4136e-04\n",
      "Epoch 111/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.3261e-04 - val_loss: 1.3962e-04\n",
      "Epoch 112/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3270e-04 - val_loss: 1.4185e-04\n",
      "Epoch 113/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.3342e-04 - val_loss: 1.3874e-04\n",
      "Epoch 114/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3174e-04 - val_loss: 1.3605e-04\n",
      "Epoch 115/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.3013e-04 - val_loss: 1.4196e-04\n",
      "Epoch 116/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.3056e-04 - val_loss: 1.3885e-04\n",
      "Epoch 117/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2953e-04 - val_loss: 1.3687e-04\n",
      "Epoch 118/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2874e-04 - val_loss: 1.3812e-04\n",
      "Epoch 119/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.2967e-04 - val_loss: 1.3891e-04\n",
      "Epoch 120/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2805e-04 - val_loss: 1.3579e-04\n",
      "Epoch 121/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2813e-04 - val_loss: 1.3372e-04\n",
      "Epoch 122/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2733e-04 - val_loss: 1.3724e-04\n",
      "Epoch 123/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2671e-04 - val_loss: 1.3928e-04\n",
      "Epoch 124/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2618e-04 - val_loss: 1.3318e-04\n",
      "Epoch 125/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2655e-04 - val_loss: 1.3542e-04\n",
      "Epoch 126/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2543e-04 - val_loss: 1.2882e-04\n",
      "Epoch 127/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.2491e-04 - val_loss: 1.4004e-04\n",
      "Epoch 128/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2450e-04 - val_loss: 1.3283e-04\n",
      "Epoch 129/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2418e-04 - val_loss: 1.3313e-04\n",
      "Epoch 130/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2394e-04 - val_loss: 1.3272e-04\n",
      "Epoch 131/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2330e-04 - val_loss: 1.3403e-04\n",
      "Epoch 132/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2334e-04 - val_loss: 1.3236e-04\n",
      "Epoch 133/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2280e-04 - val_loss: 1.3359e-04\n",
      "Epoch 134/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.2182e-04 - val_loss: 1.2983e-04\n",
      "Epoch 135/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2278e-04 - val_loss: 1.3023e-04\n",
      "Epoch 136/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2118e-04 - val_loss: 1.2727e-04\n",
      "Epoch 137/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2058e-04 - val_loss: 1.3307e-04\n",
      "Epoch 138/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.2047e-04 - val_loss: 1.3076e-04\n",
      "Epoch 139/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.2022e-04 - val_loss: 1.2982e-04\n",
      "Epoch 140/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.2022e-04 - val_loss: 1.2945e-04\n",
      "Epoch 141/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.2030e-04 - val_loss: 1.3629e-04\n",
      "Epoch 142/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1930e-04 - val_loss: 1.2290e-04\n",
      "Epoch 143/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1916e-04 - val_loss: 1.2632e-04\n",
      "Epoch 144/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1766e-04 - val_loss: 1.2304e-04\n",
      "Epoch 145/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1904e-04 - val_loss: 1.3108e-04\n",
      "Epoch 146/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1912e-04 - val_loss: 1.2481e-04\n",
      "Epoch 147/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1768e-04 - val_loss: 1.2634e-04\n",
      "Epoch 148/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1802e-04 - val_loss: 1.2531e-04\n",
      "Epoch 149/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1826e-04 - val_loss: 1.3506e-04\n",
      "Epoch 150/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1663e-04 - val_loss: 1.2181e-04\n",
      "Epoch 151/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1809e-04 - val_loss: 1.2679e-04\n",
      "Epoch 152/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1741e-04 - val_loss: 1.3253e-04\n",
      "Epoch 153/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1629e-04 - val_loss: 1.2466e-04\n",
      "Epoch 154/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1610e-04 - val_loss: 1.2896e-04\n",
      "Epoch 155/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1567e-04 - val_loss: 1.2407e-04\n",
      "Epoch 156/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1622e-04 - val_loss: 1.3109e-04\n",
      "Epoch 157/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1559e-04 - val_loss: 1.2634e-04\n",
      "Epoch 158/5000\n",
      "42880/42880 [==============================] - 1s 29us/step - loss: 1.1527e-04 - val_loss: 1.2797e-04\n",
      "Epoch 159/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.1551e-04 - val_loss: 1.2356e-04\n",
      "Epoch 160/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1463e-04 - val_loss: 1.2509e-04\n",
      "Epoch 161/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1502e-04 - val_loss: 1.2578e-04\n",
      "Epoch 162/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1486e-04 - val_loss: 1.2328e-04\n",
      "Epoch 163/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.1424e-04 - val_loss: 1.2611e-04\n",
      "Epoch 164/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1345e-04 - val_loss: 1.2356e-04\n",
      "Epoch 165/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1288e-04 - val_loss: 1.1977e-04\n",
      "Epoch 166/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1496e-04 - val_loss: 1.2711e-04\n",
      "Epoch 167/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1300e-04 - val_loss: 1.2395e-04\n",
      "Epoch 168/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1345e-04 - val_loss: 1.2109e-04\n",
      "Epoch 169/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1268e-04 - val_loss: 1.1908e-04\n",
      "Epoch 170/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.1353e-04 - val_loss: 1.2311e-04\n",
      "Epoch 171/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1305e-04 - val_loss: 1.2311e-04\n",
      "Epoch 172/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1518e-04 - val_loss: 1.2202e-04\n",
      "Epoch 173/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1274e-04 - val_loss: 1.2368e-04\n",
      "Epoch 174/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.1247e-04 - val_loss: 1.1895e-04\n",
      "Epoch 175/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1227e-04 - val_loss: 1.2167e-04\n",
      "Epoch 176/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1239e-04 - val_loss: 1.2300e-04\n",
      "Epoch 177/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1175e-04 - val_loss: 1.2137e-04\n",
      "Epoch 178/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1272e-04 - val_loss: 1.3010e-04\n",
      "Epoch 179/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1140e-04 - val_loss: 1.2161e-04\n",
      "Epoch 180/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1141e-04 - val_loss: 1.2682e-04\n",
      "Epoch 181/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1175e-04 - val_loss: 1.2037e-04\n",
      "Epoch 182/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.1056e-04 - val_loss: 1.2413e-04\n",
      "Epoch 183/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.1189e-04 - val_loss: 1.2220e-04\n",
      "Epoch 184/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.1068e-04 - val_loss: 1.1972e-04\n",
      "Epoch 185/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 1.1077e-04 - val_loss: 1.2944e-04\n",
      "Epoch 186/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1004e-04 - val_loss: 1.2151e-04\n",
      "Epoch 187/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1053e-04 - val_loss: 1.2173e-04\n",
      "Epoch 188/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0978e-04 - val_loss: 1.2590e-04\n",
      "Epoch 189/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1165e-04 - val_loss: 1.2133e-04\n",
      "Epoch 190/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1003e-04 - val_loss: 1.1874e-04\n",
      "Epoch 191/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0946e-04 - val_loss: 1.2249e-04\n",
      "Epoch 192/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.1002e-04 - val_loss: 1.2064e-04\n",
      "Epoch 193/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0896e-04 - val_loss: 1.2125e-04\n",
      "Epoch 194/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0976e-04 - val_loss: 1.2284e-04\n",
      "Epoch 195/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0985e-04 - val_loss: 1.1698e-04\n",
      "Epoch 196/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0864e-04 - val_loss: 1.3167e-04\n",
      "Epoch 197/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0860e-04 - val_loss: 1.2137e-04\n",
      "Epoch 198/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0892e-04 - val_loss: 1.2202e-04\n",
      "Epoch 199/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0784e-04 - val_loss: 1.2306e-04\n",
      "Epoch 200/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0827e-04 - val_loss: 1.2045e-04\n",
      "Epoch 201/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0787e-04 - val_loss: 1.2100e-04\n",
      "Epoch 202/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0863e-04 - val_loss: 1.1870e-04\n",
      "Epoch 203/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0853e-04 - val_loss: 1.2357e-04\n",
      "Epoch 204/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0751e-04 - val_loss: 1.2012e-04\n",
      "Epoch 205/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0823e-04 - val_loss: 1.1719e-04\n",
      "Epoch 206/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0688e-04 - val_loss: 1.1836e-04\n",
      "Epoch 207/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0691e-04 - val_loss: 1.1788e-04\n",
      "Epoch 208/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0739e-04 - val_loss: 1.1994e-04\n",
      "Epoch 209/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0855e-04 - val_loss: 1.2354e-04\n",
      "Epoch 210/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0700e-04 - val_loss: 1.1782e-04\n",
      "Epoch 211/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0650e-04 - val_loss: 1.2191e-04\n",
      "Epoch 212/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0596e-04 - val_loss: 1.2020e-04\n",
      "Epoch 213/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0668e-04 - val_loss: 1.2076e-04\n",
      "Epoch 214/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0593e-04 - val_loss: 1.1643e-04\n",
      "Epoch 215/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0679e-04 - val_loss: 1.2266e-04\n",
      "Epoch 216/5000\n",
      "42880/42880 [==============================] - ETA: 0s - loss: 1.0498e-0 - 1s 33us/step - loss: 1.0518e-04 - val_loss: 1.1618e-04\n",
      "Epoch 217/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0689e-04 - val_loss: 1.1705e-04\n",
      "Epoch 218/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0702e-04 - val_loss: 1.1969e-04\n",
      "Epoch 219/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0567e-04 - val_loss: 1.1294e-04\n",
      "Epoch 220/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0477e-04 - val_loss: 1.1654e-04\n",
      "Epoch 221/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0568e-04 - val_loss: 1.1600e-04\n",
      "Epoch 222/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0514e-04 - val_loss: 1.2138e-04\n",
      "Epoch 223/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0514e-04 - val_loss: 1.1713e-04\n",
      "Epoch 224/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0480e-04 - val_loss: 1.1530e-04\n",
      "Epoch 225/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0484e-04 - val_loss: 1.1788e-04\n",
      "Epoch 226/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0432e-04 - val_loss: 1.1591e-04\n",
      "Epoch 227/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0461e-04 - val_loss: 1.1814e-04\n",
      "Epoch 228/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 1.0447e-04 - val_loss: 1.1546e-04\n",
      "Epoch 229/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0433e-04 - val_loss: 1.2622e-04\n",
      "Epoch 230/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0451e-04 - val_loss: 1.1728e-04\n",
      "Epoch 231/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0412e-04 - val_loss: 1.1538e-04\n",
      "Epoch 232/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0313e-04 - val_loss: 1.1396e-04\n",
      "Epoch 233/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0359e-04 - val_loss: 1.1640e-04\n",
      "Epoch 234/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0382e-04 - val_loss: 1.1971e-04\n",
      "Epoch 235/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0333e-04 - val_loss: 1.1234e-04\n",
      "Epoch 236/5000\n",
      "42880/42880 [==============================] - 1s 34us/step - loss: 1.0221e-04 - val_loss: 1.1694e-04\n",
      "Epoch 237/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0312e-04 - val_loss: 1.1470e-04\n",
      "Epoch 238/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0331e-04 - val_loss: 1.1628e-04\n",
      "Epoch 239/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0275e-04 - val_loss: 1.1564e-04\n",
      "Epoch 240/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0278e-04 - val_loss: 1.1724e-04\n",
      "Epoch 241/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 1.0272e-04 - val_loss: 1.1797e-04\n",
      "Epoch 242/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0292e-04 - val_loss: 1.1291e-04\n",
      "Epoch 243/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0205e-04 - val_loss: 1.1395e-04\n",
      "Epoch 244/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0400e-04 - val_loss: 1.1280e-04\n",
      "Epoch 245/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0137e-04 - val_loss: 1.1653e-04\n",
      "Epoch 246/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0183e-04 - val_loss: 1.1327e-04\n",
      "Epoch 247/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0176e-04 - val_loss: 1.1228e-04\n",
      "Epoch 248/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9936e-05 - val_loss: 1.1505e-04\n",
      "Epoch 249/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0345e-04 - val_loss: 1.1277e-04\n",
      "Epoch 250/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 1.0078e-04 - val_loss: 1.1867e-04\n",
      "Epoch 251/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9784e-05 - val_loss: 1.1035e-04\n",
      "Epoch 252/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.8935e-05 - val_loss: 1.1395e-04\n",
      "Epoch 253/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9243e-05 - val_loss: 1.1064e-04\n",
      "Epoch 254/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9579e-05 - val_loss: 1.1297e-04\n",
      "Epoch 255/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9169e-05 - val_loss: 1.1862e-04\n",
      "Epoch 256/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.8623e-05 - val_loss: 1.1127e-04\n",
      "Epoch 257/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9188e-05 - val_loss: 1.1247e-04\n",
      "Epoch 258/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.9063e-05 - val_loss: 1.1261e-04\n",
      "Epoch 259/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.8418e-05 - val_loss: 1.1426e-04\n",
      "Epoch 260/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.7838e-05 - val_loss: 1.1540e-04\n",
      "Epoch 261/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.8654e-05 - val_loss: 1.1102e-04\n",
      "Epoch 262/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.7652e-05 - val_loss: 1.1545e-04\n",
      "Epoch 263/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 9.7353e-05 - val_loss: 1.1562e-04\n",
      "Epoch 264/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.8144e-05 - val_loss: 1.1086e-04\n",
      "Epoch 265/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.6573e-05 - val_loss: 1.1487e-04\n",
      "Epoch 266/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 9.8363e-05 - val_loss: 1.1572e-04\n",
      "Epoch 267/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.6442e-05 - val_loss: 1.0936e-04\n",
      "Epoch 268/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.6102e-05 - val_loss: 1.1531e-04\n",
      "Epoch 269/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.7659e-05 - val_loss: 1.0757e-04\n",
      "Epoch 270/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.6012e-05 - val_loss: 1.1142e-04\n",
      "Epoch 271/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5820e-05 - val_loss: 1.1004e-04\n",
      "Epoch 272/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.6900e-05 - val_loss: 1.1329e-04\n",
      "Epoch 273/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.5631e-05 - val_loss: 1.1264e-04\n",
      "Epoch 274/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.7990e-05 - val_loss: 1.1242e-04\n",
      "Epoch 275/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5990e-05 - val_loss: 1.1344e-04\n",
      "Epoch 276/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.5534e-05 - val_loss: 1.1168e-04\n",
      "Epoch 277/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.6088e-05 - val_loss: 1.1098e-04\n",
      "Epoch 278/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5521e-05 - val_loss: 1.1100e-04\n",
      "Epoch 279/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5890e-05 - val_loss: 1.1245e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.5664e-05 - val_loss: 1.1198e-04\n",
      "Epoch 281/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.4816e-05 - val_loss: 1.1041e-04\n",
      "Epoch 282/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.6750e-05 - val_loss: 1.1220e-04\n",
      "Epoch 283/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4913e-05 - val_loss: 1.0901e-04\n",
      "Epoch 284/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5417e-05 - val_loss: 1.0724e-04\n",
      "Epoch 285/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5932e-05 - val_loss: 1.1389e-04\n",
      "Epoch 286/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5450e-05 - val_loss: 1.0746e-04\n",
      "Epoch 287/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4785e-05 - val_loss: 1.0869e-04\n",
      "Epoch 288/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4129e-05 - val_loss: 1.1048e-04\n",
      "Epoch 289/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.4287e-05 - val_loss: 1.1155e-04\n",
      "Epoch 290/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4662e-05 - val_loss: 1.1103e-04\n",
      "Epoch 291/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4970e-05 - val_loss: 1.1224e-04\n",
      "Epoch 292/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.5156e-05 - val_loss: 1.1052e-04\n",
      "Epoch 293/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 9.4392e-05 - val_loss: 1.0809e-04\n",
      "Epoch 294/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 9.3835e-05 - val_loss: 1.0853e-04\n",
      "Epoch 295/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.4492e-05 - val_loss: 1.0524e-04\n",
      "Epoch 296/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 9.3969e-05 - val_loss: 1.0688e-04\n",
      "Epoch 297/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 9.3712e-05 - val_loss: 1.0783e-04\n",
      "Epoch 298/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.3004e-05 - val_loss: 1.1128e-04\n",
      "Epoch 299/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 9.4013e-05 - val_loss: 1.0650e-04\n",
      "Epoch 300/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.6322e-05 - val_loss: 1.1264e-04\n",
      "Epoch 301/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.4240e-05 - val_loss: 1.0912e-04\n",
      "Epoch 302/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2516e-05 - val_loss: 1.0728e-04\n",
      "Epoch 303/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.3179e-05 - val_loss: 1.0573e-04\n",
      "Epoch 304/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2368e-05 - val_loss: 1.0450e-04\n",
      "Epoch 305/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.3619e-05 - val_loss: 1.0879e-04\n",
      "Epoch 306/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.2080e-05 - val_loss: 1.0645e-04\n",
      "Epoch 307/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.2503e-05 - val_loss: 1.1145e-04\n",
      "Epoch 308/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.3788e-05 - val_loss: 1.0828e-04\n",
      "Epoch 309/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2522e-05 - val_loss: 1.0606e-04\n",
      "Epoch 310/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.2853e-05 - val_loss: 1.0773e-04\n",
      "Epoch 311/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.2250e-05 - val_loss: 1.0644e-04\n",
      "Epoch 312/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2402e-05 - val_loss: 1.0957e-04\n",
      "Epoch 313/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2323e-05 - val_loss: 1.0794e-04\n",
      "Epoch 314/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.2233e-05 - val_loss: 1.0719e-04\n",
      "Epoch 315/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1703e-05 - val_loss: 1.1532e-04\n",
      "Epoch 316/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.1995e-05 - val_loss: 1.0615e-04\n",
      "Epoch 317/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.1662e-05 - val_loss: 1.1006e-04\n",
      "Epoch 318/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.1400e-05 - val_loss: 1.0342e-04\n",
      "Epoch 319/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1561e-05 - val_loss: 1.0920e-04\n",
      "Epoch 320/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.1908e-05 - val_loss: 1.0732e-04\n",
      "Epoch 321/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1449e-05 - val_loss: 1.1090e-04\n",
      "Epoch 322/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1080e-05 - val_loss: 1.0613e-04\n",
      "Epoch 323/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0952e-05 - val_loss: 1.0828e-04\n",
      "Epoch 324/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0757e-05 - val_loss: 1.0281e-04\n",
      "Epoch 325/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0972e-05 - val_loss: 1.0879e-04\n",
      "Epoch 326/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0663e-05 - val_loss: 1.0380e-04\n",
      "Epoch 327/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0942e-05 - val_loss: 1.0969e-04\n",
      "Epoch 328/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0610e-05 - val_loss: 1.0831e-04\n",
      "Epoch 329/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.1094e-05 - val_loss: 1.0680e-04\n",
      "Epoch 330/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1097e-05 - val_loss: 1.0545e-04\n",
      "Epoch 331/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0596e-05 - val_loss: 1.0453e-04\n",
      "Epoch 332/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0287e-05 - val_loss: 1.1002e-04\n",
      "Epoch 333/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0026e-05 - val_loss: 1.0305e-04\n",
      "Epoch 334/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0260e-05 - val_loss: 1.0589e-04\n",
      "Epoch 335/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0198e-05 - val_loss: 1.0518e-04\n",
      "Epoch 336/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.0094e-05 - val_loss: 1.0583e-04\n",
      "Epoch 337/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9585e-05 - val_loss: 1.0492e-04\n",
      "Epoch 338/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9511e-05 - val_loss: 1.0614e-04\n",
      "Epoch 339/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2784e-05 - val_loss: 1.0140e-04\n",
      "Epoch 340/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.9614e-05 - val_loss: 1.0752e-04\n",
      "Epoch 341/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9317e-05 - val_loss: 1.0222e-04\n",
      "Epoch 342/5000\n",
      "42880/42880 [==============================] - 1s 29us/step - loss: 8.9069e-05 - val_loss: 1.0399e-04\n",
      "Epoch 343/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8981e-05 - val_loss: 1.0298e-04\n",
      "Epoch 344/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9247e-05 - val_loss: 1.0224e-04\n",
      "Epoch 345/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9372e-05 - val_loss: 1.0269e-04\n",
      "Epoch 346/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8149e-05 - val_loss: 1.0703e-04\n",
      "Epoch 347/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8867e-05 - val_loss: 1.0586e-04\n",
      "Epoch 348/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9102e-05 - val_loss: 1.0206e-04\n",
      "Epoch 349/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9410e-05 - val_loss: 1.0812e-04\n",
      "Epoch 350/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.8674e-05 - val_loss: 1.0843e-04\n",
      "Epoch 351/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8483e-05 - val_loss: 1.0172e-04\n",
      "Epoch 352/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.2828e-05 - val_loss: 1.0737e-04\n",
      "Epoch 353/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9119e-05 - val_loss: 1.0198e-04\n",
      "Epoch 354/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7774e-05 - val_loss: 1.1088e-04\n",
      "Epoch 355/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8170e-05 - val_loss: 1.0554e-04\n",
      "Epoch 356/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8636e-05 - val_loss: 1.0143e-04\n",
      "Epoch 357/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.9286e-05 - val_loss: 1.0168e-04\n",
      "Epoch 358/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8130e-05 - val_loss: 1.0160e-04\n",
      "Epoch 359/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8127e-05 - val_loss: 1.0500e-04\n",
      "Epoch 360/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7963e-05 - val_loss: 1.0108e-04\n",
      "Epoch 361/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7790e-05 - val_loss: 1.0532e-04\n",
      "Epoch 362/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7628e-05 - val_loss: 1.0418e-04\n",
      "Epoch 363/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8384e-05 - val_loss: 1.0490e-04\n",
      "Epoch 364/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7563e-05 - val_loss: 1.0277e-04\n",
      "Epoch 365/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.9135e-05 - val_loss: 1.0219e-04\n",
      "Epoch 366/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8532e-05 - val_loss: 1.0117e-04\n",
      "Epoch 367/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7891e-05 - val_loss: 1.0211e-04\n",
      "Epoch 368/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8277e-05 - val_loss: 1.0045e-04\n",
      "Epoch 369/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6568e-05 - val_loss: 1.0151e-04\n",
      "Epoch 370/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7392e-05 - val_loss: 1.0573e-04\n",
      "Epoch 371/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.7755e-05 - val_loss: 1.0430e-04\n",
      "Epoch 372/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.7364e-05 - val_loss: 1.0546e-04\n",
      "Epoch 373/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7642e-05 - val_loss: 1.0086e-04\n",
      "Epoch 374/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7312e-05 - val_loss: 9.9670e-05\n",
      "Epoch 375/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6744e-05 - val_loss: 1.0493e-04\n",
      "Epoch 376/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.8009e-05 - val_loss: 1.0138e-04\n",
      "Epoch 377/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6893e-05 - val_loss: 9.8254e-05\n",
      "Epoch 378/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7039e-05 - val_loss: 9.7420e-05\n",
      "Epoch 379/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.6709e-05 - val_loss: 1.0136e-04\n",
      "Epoch 380/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.7543e-05 - val_loss: 9.8851e-05\n",
      "Epoch 381/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.8428e-05 - val_loss: 1.0093e-04\n",
      "Epoch 382/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7004e-05 - val_loss: 9.9445e-05\n",
      "Epoch 383/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6569e-05 - val_loss: 1.0547e-04\n",
      "Epoch 384/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.7129e-05 - val_loss: 9.9823e-05\n",
      "Epoch 385/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5908e-05 - val_loss: 1.0431e-04\n",
      "Epoch 386/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6027e-05 - val_loss: 1.0195e-04\n",
      "Epoch 387/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6609e-05 - val_loss: 1.0309e-04\n",
      "Epoch 388/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7016e-05 - val_loss: 9.9755e-05\n",
      "Epoch 389/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6543e-05 - val_loss: 1.0167e-04\n",
      "Epoch 390/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7100e-05 - val_loss: 1.0772e-04\n",
      "Epoch 391/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5999e-05 - val_loss: 9.9859e-05\n",
      "Epoch 392/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.6068e-05 - val_loss: 9.9705e-05\n",
      "Epoch 393/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6679e-05 - val_loss: 9.8385e-05\n",
      "Epoch 394/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.6080e-05 - val_loss: 9.9537e-05\n",
      "Epoch 395/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5749e-05 - val_loss: 9.9168e-05\n",
      "Epoch 396/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5890e-05 - val_loss: 1.0084e-04\n",
      "Epoch 397/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5735e-05 - val_loss: 1.0265e-04\n",
      "Epoch 398/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6600e-05 - val_loss: 9.8775e-05\n",
      "Epoch 399/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5673e-05 - val_loss: 9.8874e-05\n",
      "Epoch 400/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5988e-05 - val_loss: 1.0447e-04\n",
      "Epoch 401/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6170e-05 - val_loss: 9.9005e-05\n",
      "Epoch 402/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.6556e-05 - val_loss: 9.8667e-05\n",
      "Epoch 403/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5559e-05 - val_loss: 9.8775e-05\n",
      "Epoch 404/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4477e-05 - val_loss: 9.7037e-05\n",
      "Epoch 405/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6933e-05 - val_loss: 9.7072e-05\n",
      "Epoch 406/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5723e-05 - val_loss: 9.9598e-05\n",
      "Epoch 407/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.5009e-05 - val_loss: 9.7308e-05\n",
      "Epoch 408/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5317e-05 - val_loss: 1.0076e-04\n",
      "Epoch 409/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.5583e-05 - val_loss: 9.7432e-05\n",
      "Epoch 410/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.6010e-05 - val_loss: 9.4946e-05\n",
      "Epoch 411/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.4756e-05 - val_loss: 9.7970e-05\n",
      "Epoch 412/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.5265e-05 - val_loss: 9.8400e-05\n",
      "Epoch 413/5000\n",
      "42880/42880 [==============================] - 2s 37us/step - loss: 8.5497e-05 - val_loss: 9.8763e-05\n",
      "Epoch 414/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5838e-05 - val_loss: 9.7705e-05\n",
      "Epoch 415/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 9.0268e-05 - val_loss: 9.7570e-05\n",
      "Epoch 416/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.4577e-05 - val_loss: 9.7474e-05\n",
      "Epoch 417/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4481e-05 - val_loss: 1.0384e-04\n",
      "Epoch 418/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.5770e-05 - val_loss: 9.6863e-05\n",
      "Epoch 419/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4319e-05 - val_loss: 9.7471e-05\n",
      "Epoch 420/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5273e-05 - val_loss: 1.0002e-04\n",
      "Epoch 421/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5562e-05 - val_loss: 9.8884e-05\n",
      "Epoch 422/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5132e-05 - val_loss: 9.9381e-05\n",
      "Epoch 423/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5361e-05 - val_loss: 1.0101e-04\n",
      "Epoch 424/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5011e-05 - val_loss: 9.8251e-05\n",
      "Epoch 425/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4939e-05 - val_loss: 9.9857e-05\n",
      "Epoch 426/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.5428e-05 - val_loss: 9.7888e-05\n",
      "Epoch 427/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4739e-05 - val_loss: 1.0121e-04\n",
      "Epoch 428/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4179e-05 - val_loss: 1.0017e-04\n",
      "Epoch 429/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4806e-05 - val_loss: 9.4022e-05\n",
      "Epoch 430/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.7570e-05 - val_loss: 9.6933e-05\n",
      "Epoch 431/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4591e-05 - val_loss: 9.9781e-05\n",
      "Epoch 432/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4786e-05 - val_loss: 9.7592e-05\n",
      "Epoch 433/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3864e-05 - val_loss: 9.5814e-05\n",
      "Epoch 434/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3746e-05 - val_loss: 9.5840e-05\n",
      "Epoch 435/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4576e-05 - val_loss: 9.9195e-05\n",
      "Epoch 436/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.9385e-05 - val_loss: 9.9434e-05\n",
      "Epoch 437/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4866e-05 - val_loss: 1.0010e-04\n",
      "Epoch 438/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3150e-05 - val_loss: 9.8376e-05\n",
      "Epoch 439/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.4364e-05 - val_loss: 9.5334e-05\n",
      "Epoch 440/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3854e-05 - val_loss: 9.7999e-05\n",
      "Epoch 441/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4117e-05 - val_loss: 9.6857e-05\n",
      "Epoch 442/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4974e-05 - val_loss: 9.7398e-05\n",
      "Epoch 443/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4393e-05 - val_loss: 9.5268e-05\n",
      "Epoch 444/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3987e-05 - val_loss: 1.0115e-04\n",
      "Epoch 445/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.3383e-05 - val_loss: 9.6846e-05\n",
      "Epoch 446/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.3604e-05 - val_loss: 9.6303e-05\n",
      "Epoch 447/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3210e-05 - val_loss: 9.7866e-05\n",
      "Epoch 448/5000\n",
      "42880/42880 [==============================] - 1s 29us/step - loss: 8.3347e-05 - val_loss: 9.6741e-05\n",
      "Epoch 449/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.3597e-05 - val_loss: 9.6070e-05\n",
      "Epoch 450/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.4028e-05 - val_loss: 9.7062e-05\n",
      "Epoch 451/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3902e-05 - val_loss: 9.6201e-05\n",
      "Epoch 452/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.4517e-05 - val_loss: 9.7263e-05\n",
      "Epoch 453/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4092e-05 - val_loss: 9.6339e-05\n",
      "Epoch 454/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3197e-05 - val_loss: 9.5654e-05\n",
      "Epoch 455/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3519e-05 - val_loss: 9.6377e-05\n",
      "Epoch 456/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3781e-05 - val_loss: 9.6824e-05\n",
      "Epoch 457/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3226e-05 - val_loss: 9.6057e-05\n",
      "Epoch 458/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2770e-05 - val_loss: 9.4920e-05\n",
      "Epoch 459/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3268e-05 - val_loss: 9.5277e-05\n",
      "Epoch 460/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3484e-05 - val_loss: 9.5481e-05\n",
      "Epoch 461/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3435e-05 - val_loss: 9.4175e-05\n",
      "Epoch 462/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3915e-05 - val_loss: 9.8238e-05\n",
      "Epoch 463/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3658e-05 - val_loss: 9.6321e-05\n",
      "Epoch 464/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3120e-05 - val_loss: 9.5131e-05\n",
      "Epoch 465/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.4088e-05 - val_loss: 9.6141e-05\n",
      "Epoch 466/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.3669e-05 - val_loss: 9.5347e-05\n",
      "Epoch 467/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2947e-05 - val_loss: 9.2118e-05\n",
      "Epoch 468/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2749e-05 - val_loss: 9.5101e-05\n",
      "Epoch 469/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.3169e-05 - val_loss: 9.2010e-05\n",
      "Epoch 470/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3372e-05 - val_loss: 9.7861e-05\n",
      "Epoch 471/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3169e-05 - val_loss: 9.4471e-05\n",
      "Epoch 472/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2867e-05 - val_loss: 9.3850e-05\n",
      "Epoch 473/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1822e-05 - val_loss: 1.0123e-04\n",
      "Epoch 474/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2446e-05 - val_loss: 9.8562e-05\n",
      "Epoch 475/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2371e-05 - val_loss: 9.3145e-05\n",
      "Epoch 476/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2281e-05 - val_loss: 9.2666e-05\n",
      "Epoch 477/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1805e-05 - val_loss: 9.5666e-05\n",
      "Epoch 478/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 9.1005e-05 - val_loss: 9.7573e-05\n",
      "Epoch 479/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.5151e-05 - val_loss: 9.7822e-05\n",
      "Epoch 480/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2321e-05 - val_loss: 9.8447e-05\n",
      "Epoch 481/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3414e-05 - val_loss: 9.4251e-05\n",
      "Epoch 482/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2103e-05 - val_loss: 9.4931e-05\n",
      "Epoch 483/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.1956e-05 - val_loss: 9.6854e-05\n",
      "Epoch 484/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.3379e-05 - val_loss: 1.0378e-04\n",
      "Epoch 485/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2236e-05 - val_loss: 9.5982e-05\n",
      "Epoch 486/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2142e-05 - val_loss: 9.7896e-05\n",
      "Epoch 487/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2182e-05 - val_loss: 9.4526e-05\n",
      "Epoch 488/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1777e-05 - val_loss: 9.5596e-05\n",
      "Epoch 489/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2240e-05 - val_loss: 9.4563e-05\n",
      "Epoch 490/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2425e-05 - val_loss: 1.0295e-04\n",
      "Epoch 491/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3914e-05 - val_loss: 9.6195e-05\n",
      "Epoch 492/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1079e-05 - val_loss: 9.3239e-05\n",
      "Epoch 493/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2378e-05 - val_loss: 9.3474e-05\n",
      "Epoch 494/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2029e-05 - val_loss: 9.5135e-05\n",
      "Epoch 495/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.1901e-05 - val_loss: 9.4116e-05\n",
      "Epoch 496/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1797e-05 - val_loss: 9.4774e-05\n",
      "Epoch 497/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1385e-05 - val_loss: 9.2086e-05\n",
      "Epoch 498/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1282e-05 - val_loss: 9.2641e-05\n",
      "Epoch 499/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1009e-05 - val_loss: 9.2646e-05\n",
      "Epoch 500/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.2785e-05 - val_loss: 9.1971e-05\n",
      "Epoch 501/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2009e-05 - val_loss: 9.5570e-05\n",
      "Epoch 502/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1836e-05 - val_loss: 9.7083e-05\n",
      "Epoch 503/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1025e-05 - val_loss: 9.4384e-05\n",
      "Epoch 504/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0888e-05 - val_loss: 9.4880e-05\n",
      "Epoch 505/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1484e-05 - val_loss: 9.3208e-05\n",
      "Epoch 506/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1248e-05 - val_loss: 9.4532e-05\n",
      "Epoch 507/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.1259e-05 - val_loss: 9.7922e-05\n",
      "Epoch 508/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1951e-05 - val_loss: 9.1484e-05\n",
      "Epoch 509/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 8.0706e-05 - val_loss: 9.3594e-05\n",
      "Epoch 510/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0649e-05 - val_loss: 9.3544e-05\n",
      "Epoch 511/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0206e-05 - val_loss: 9.7672e-05\n",
      "Epoch 512/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2147e-05 - val_loss: 9.6498e-05\n",
      "Epoch 513/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9945e-05 - val_loss: 9.2201e-05\n",
      "Epoch 514/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1383e-05 - val_loss: 9.4753e-05\n",
      "Epoch 515/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0392e-05 - val_loss: 9.2090e-05\n",
      "Epoch 516/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2435e-05 - val_loss: 9.3638e-05\n",
      "Epoch 517/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.1107e-05 - val_loss: 9.3036e-05\n",
      "Epoch 518/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0794e-05 - val_loss: 9.1917e-05\n",
      "Epoch 519/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0273e-05 - val_loss: 9.6490e-05\n",
      "Epoch 520/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0246e-05 - val_loss: 9.3313e-05\n",
      "Epoch 521/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.9632e-05 - val_loss: 1.0336e-04\n",
      "Epoch 522/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.2276e-05 - val_loss: 9.3218e-05\n",
      "Epoch 523/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 8.0193e-05 - val_loss: 9.1572e-05\n",
      "Epoch 524/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9171e-05 - val_loss: 9.6421e-05\n",
      "Epoch 525/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8897e-05 - val_loss: 9.2052e-05\n",
      "Epoch 526/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0007e-05 - val_loss: 9.3806e-05\n",
      "Epoch 527/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9814e-05 - val_loss: 9.4544e-05\n",
      "Epoch 528/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9726e-05 - val_loss: 9.5554e-05\n",
      "Epoch 529/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9375e-05 - val_loss: 9.2803e-05\n",
      "Epoch 530/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9222e-05 - val_loss: 9.0735e-05\n",
      "Epoch 531/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 7.9232e-05 - val_loss: 9.4143e-05\n",
      "Epoch 532/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0533e-05 - val_loss: 9.3848e-05\n",
      "Epoch 533/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9920e-05 - val_loss: 9.3221e-05\n",
      "Epoch 534/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9043e-05 - val_loss: 9.3485e-05\n",
      "Epoch 535/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9472e-05 - val_loss: 9.4147e-05\n",
      "Epoch 536/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9386e-05 - val_loss: 9.1872e-05\n",
      "Epoch 537/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9962e-05 - val_loss: 9.4177e-05\n",
      "Epoch 538/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9149e-05 - val_loss: 9.2554e-05\n",
      "Epoch 539/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9373e-05 - val_loss: 9.3339e-05\n",
      "Epoch 540/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9948e-05 - val_loss: 9.2075e-05\n",
      "Epoch 541/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0386e-05 - val_loss: 9.9071e-05\n",
      "Epoch 542/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0464e-05 - val_loss: 9.8544e-05\n",
      "Epoch 543/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0637e-05 - val_loss: 9.4598e-05\n",
      "Epoch 544/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8906e-05 - val_loss: 9.1287e-05\n",
      "Epoch 545/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8205e-05 - val_loss: 8.9878e-05\n",
      "Epoch 546/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8927e-05 - val_loss: 9.2464e-05\n",
      "Epoch 547/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8285e-05 - val_loss: 9.1468e-05\n",
      "Epoch 548/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 7.8577e-05 - val_loss: 9.8922e-05\n",
      "Epoch 549/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0122e-05 - val_loss: 9.1241e-05\n",
      "Epoch 550/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8915e-05 - val_loss: 9.3950e-05\n",
      "Epoch 551/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9264e-05 - val_loss: 9.1870e-05\n",
      "Epoch 552/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9130e-05 - val_loss: 9.1969e-05\n",
      "Epoch 553/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9728e-05 - val_loss: 9.1000e-05\n",
      "Epoch 554/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7824e-05 - val_loss: 8.9969e-05\n",
      "Epoch 555/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8836e-05 - val_loss: 9.4571e-05\n",
      "Epoch 556/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9237e-05 - val_loss: 9.2917e-05\n",
      "Epoch 557/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8290e-05 - val_loss: 9.4024e-05\n",
      "Epoch 558/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8139e-05 - val_loss: 9.0651e-05\n",
      "Epoch 559/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9947e-05 - val_loss: 9.3355e-05\n",
      "Epoch 560/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8873e-05 - val_loss: 9.1837e-05\n",
      "Epoch 561/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.9885e-05 - val_loss: 8.8997e-05\n",
      "Epoch 562/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8038e-05 - val_loss: 8.9495e-05\n",
      "Epoch 563/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8141e-05 - val_loss: 8.9230e-05\n",
      "Epoch 564/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8104e-05 - val_loss: 9.0583e-05\n",
      "Epoch 565/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8917e-05 - val_loss: 8.8314e-05\n",
      "Epoch 566/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.9305e-05 - val_loss: 9.3068e-05\n",
      "Epoch 567/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8211e-05 - val_loss: 9.2609e-05\n",
      "Epoch 568/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8597e-05 - val_loss: 9.4006e-05\n",
      "Epoch 569/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8358e-05 - val_loss: 9.0811e-05\n",
      "Epoch 570/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8665e-05 - val_loss: 9.2448e-05\n",
      "Epoch 571/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8110e-05 - val_loss: 9.4360e-05\n",
      "Epoch 572/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8535e-05 - val_loss: 1.0569e-04\n",
      "Epoch 573/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8671e-05 - val_loss: 8.9547e-05\n",
      "Epoch 574/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8110e-05 - val_loss: 9.2122e-05\n",
      "Epoch 575/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7534e-05 - val_loss: 9.0852e-05\n",
      "Epoch 576/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7324e-05 - val_loss: 8.9312e-05\n",
      "Epoch 577/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8910e-05 - val_loss: 9.0213e-05\n",
      "Epoch 578/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8915e-05 - val_loss: 9.0397e-05\n",
      "Epoch 579/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.0753e-05 - val_loss: 9.1182e-05\n",
      "Epoch 580/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7920e-05 - val_loss: 9.4595e-05\n",
      "Epoch 581/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7742e-05 - val_loss: 9.0824e-05\n",
      "Epoch 582/5000\n",
      "42880/42880 [==============================] - 1s 33us/step - loss: 7.7803e-05 - val_loss: 9.2325e-05\n",
      "Epoch 583/5000\n",
      "42880/42880 [==============================] - 1s 30us/step - loss: 7.7486e-05 - val_loss: 9.9661e-05\n",
      "Epoch 584/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7817e-05 - val_loss: 9.2137e-05\n",
      "Epoch 585/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8100e-05 - val_loss: 9.3168e-05\n",
      "Epoch 586/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8022e-05 - val_loss: 8.9739e-05\n",
      "Epoch 587/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8138e-05 - val_loss: 9.2548e-05\n",
      "Epoch 588/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7233e-05 - val_loss: 9.0120e-05\n",
      "Epoch 589/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7475e-05 - val_loss: 9.9763e-05\n",
      "Epoch 590/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7869e-05 - val_loss: 9.7672e-05\n",
      "Epoch 591/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8654e-05 - val_loss: 8.7670e-05\n",
      "Epoch 592/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7614e-05 - val_loss: 9.0693e-05\n",
      "Epoch 593/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7758e-05 - val_loss: 9.0002e-05\n",
      "Epoch 594/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7379e-05 - val_loss: 9.3464e-05\n",
      "Epoch 595/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 8.0330e-05 - val_loss: 9.2970e-05\n",
      "Epoch 596/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6893e-05 - val_loss: 9.0350e-05\n",
      "Epoch 597/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6704e-05 - val_loss: 8.8713e-05\n",
      "Epoch 598/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7827e-05 - val_loss: 8.6089e-05\n",
      "Epoch 599/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7702e-05 - val_loss: 8.9598e-05\n",
      "Epoch 600/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6647e-05 - val_loss: 8.9767e-05\n",
      "Epoch 601/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8542e-05 - val_loss: 9.1049e-05\n",
      "Epoch 602/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6971e-05 - val_loss: 8.7895e-05\n",
      "Epoch 603/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7193e-05 - val_loss: 9.6867e-05\n",
      "Epoch 604/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6997e-05 - val_loss: 8.7874e-05\n",
      "Epoch 605/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7246e-05 - val_loss: 8.9177e-05\n",
      "Epoch 606/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7474e-05 - val_loss: 8.8948e-05\n",
      "Epoch 607/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7298e-05 - val_loss: 8.9841e-05\n",
      "Epoch 608/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7024e-05 - val_loss: 8.5612e-05\n",
      "Epoch 609/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8447e-05 - val_loss: 9.9057e-05\n",
      "Epoch 610/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.8264e-05 - val_loss: 9.2179e-05\n",
      "Epoch 611/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6278e-05 - val_loss: 8.9795e-05\n",
      "Epoch 612/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7351e-05 - val_loss: 9.2870e-05\n",
      "Epoch 613/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6416e-05 - val_loss: 8.7837e-05\n",
      "Epoch 614/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7174e-05 - val_loss: 8.8245e-05\n",
      "Epoch 615/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6890e-05 - val_loss: 9.0175e-05\n",
      "Epoch 616/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6429e-05 - val_loss: 9.0413e-05\n",
      "Epoch 617/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8343e-05 - val_loss: 8.9461e-05\n",
      "Epoch 618/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6438e-05 - val_loss: 9.3979e-05\n",
      "Epoch 619/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7398e-05 - val_loss: 8.8693e-05\n",
      "Epoch 620/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6612e-05 - val_loss: 9.2009e-05\n",
      "Epoch 621/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8217e-05 - val_loss: 9.1640e-05\n",
      "Epoch 622/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6814e-05 - val_loss: 8.9061e-05\n",
      "Epoch 623/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6616e-05 - val_loss: 9.6268e-05\n",
      "Epoch 624/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7970e-05 - val_loss: 8.8935e-05\n",
      "Epoch 625/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 8.3275e-05 - val_loss: 1.0707e-04\n",
      "Epoch 626/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.8268e-05 - val_loss: 8.9285e-05\n",
      "Epoch 627/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6638e-05 - val_loss: 8.6777e-05\n",
      "Epoch 628/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6131e-05 - val_loss: 8.7632e-05\n",
      "Epoch 629/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6033e-05 - val_loss: 8.6535e-05\n",
      "Epoch 630/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6592e-05 - val_loss: 8.9502e-05\n",
      "Epoch 631/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5888e-05 - val_loss: 8.7478e-05\n",
      "Epoch 632/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6695e-05 - val_loss: 8.6398e-05\n",
      "Epoch 633/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6352e-05 - val_loss: 8.6241e-05\n",
      "Epoch 634/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6346e-05 - val_loss: 8.6790e-05\n",
      "Epoch 635/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7230e-05 - val_loss: 8.8649e-05\n",
      "Epoch 636/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7338e-05 - val_loss: 8.8463e-05\n",
      "Epoch 637/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7023e-05 - val_loss: 8.7531e-05\n",
      "Epoch 638/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6050e-05 - val_loss: 8.5699e-05\n",
      "Epoch 639/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6088e-05 - val_loss: 9.1832e-05\n",
      "Epoch 640/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6265e-05 - val_loss: 8.9904e-05\n",
      "Epoch 641/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6765e-05 - val_loss: 9.0002e-05\n",
      "Epoch 642/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.7177e-05 - val_loss: 8.5372e-05\n",
      "Epoch 643/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6388e-05 - val_loss: 8.8889e-05\n",
      "Epoch 644/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6117e-05 - val_loss: 8.9246e-05\n",
      "Epoch 645/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6927e-05 - val_loss: 8.4203e-05\n",
      "Epoch 646/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6747e-05 - val_loss: 8.8622e-05\n",
      "Epoch 647/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5633e-05 - val_loss: 8.7613e-05\n",
      "Epoch 648/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6154e-05 - val_loss: 8.8865e-05\n",
      "Epoch 649/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6428e-05 - val_loss: 9.0014e-05\n",
      "Epoch 650/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6277e-05 - val_loss: 9.3115e-05\n",
      "Epoch 651/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6130e-05 - val_loss: 8.7696e-05\n",
      "Epoch 652/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5002e-05 - val_loss: 8.7957e-05\n",
      "Epoch 653/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6299e-05 - val_loss: 8.7599e-05\n",
      "Epoch 654/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6112e-05 - val_loss: 8.7582e-05\n",
      "Epoch 655/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6950e-05 - val_loss: 9.1359e-05\n",
      "Epoch 656/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6009e-05 - val_loss: 9.0426e-05\n",
      "Epoch 657/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6344e-05 - val_loss: 9.3738e-05\n",
      "Epoch 658/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5762e-05 - val_loss: 8.8681e-05\n",
      "Epoch 659/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5699e-05 - val_loss: 8.8773e-05\n",
      "Epoch 660/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5656e-05 - val_loss: 8.9254e-05\n",
      "Epoch 661/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5454e-05 - val_loss: 8.8677e-05\n",
      "Epoch 662/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5970e-05 - val_loss: 9.2270e-05\n",
      "Epoch 663/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6882e-05 - val_loss: 9.1474e-05\n",
      "Epoch 664/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6039e-05 - val_loss: 9.0154e-05\n",
      "Epoch 665/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6757e-05 - val_loss: 9.1810e-05\n",
      "Epoch 666/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6426e-05 - val_loss: 8.8372e-05\n",
      "Epoch 667/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5802e-05 - val_loss: 8.5812e-05\n",
      "Epoch 668/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5289e-05 - val_loss: 9.0929e-05\n",
      "Epoch 669/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6004e-05 - val_loss: 8.9362e-05\n",
      "Epoch 670/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5785e-05 - val_loss: 9.0898e-05\n",
      "Epoch 671/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5331e-05 - val_loss: 8.6801e-05\n",
      "Epoch 672/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5355e-05 - val_loss: 8.7221e-05\n",
      "Epoch 673/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5405e-05 - val_loss: 8.7334e-05\n",
      "Epoch 674/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.4679e-05 - val_loss: 8.8018e-05\n",
      "Epoch 675/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5742e-05 - val_loss: 8.9580e-05\n",
      "Epoch 676/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6495e-05 - val_loss: 9.8120e-05\n",
      "Epoch 677/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.7641e-05 - val_loss: 9.0014e-05\n",
      "Epoch 678/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6303e-05 - val_loss: 8.6889e-05\n",
      "Epoch 679/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6435e-05 - val_loss: 8.9963e-05\n",
      "Epoch 680/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5401e-05 - val_loss: 8.7373e-05\n",
      "Epoch 681/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5016e-05 - val_loss: 8.5594e-05\n",
      "Epoch 682/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.6192e-05 - val_loss: 9.2228e-05\n",
      "Epoch 683/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5704e-05 - val_loss: 8.9667e-05\n",
      "Epoch 684/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.4975e-05 - val_loss: 8.7923e-05\n",
      "Epoch 685/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5165e-05 - val_loss: 8.8284e-05\n",
      "Epoch 686/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6120e-05 - val_loss: 9.2926e-05\n",
      "Epoch 687/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5917e-05 - val_loss: 8.5057e-05\n",
      "Epoch 688/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6784e-05 - val_loss: 9.0727e-05\n",
      "Epoch 689/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5485e-05 - val_loss: 8.9476e-05\n",
      "Epoch 690/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5566e-05 - val_loss: 8.8333e-05\n",
      "Epoch 691/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5000e-05 - val_loss: 8.6364e-05\n",
      "Epoch 692/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.5695e-05 - val_loss: 9.2761e-05\n",
      "Epoch 693/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5129e-05 - val_loss: 9.0956e-05\n",
      "Epoch 694/5000\n",
      "42880/42880 [==============================] - 1s 31us/step - loss: 7.6122e-05 - val_loss: 8.9137e-05\n",
      "Epoch 695/5000\n",
      "42880/42880 [==============================] - 1s 32us/step - loss: 7.5041e-05 - val_loss: 8.7057e-05\n",
      " 7520/11038 [===================>..........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83625\\DataIDPS\\Autoencoder\\NSL_KDD_AE_V2.py:95: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pack_attack = pack_attack / np.max(pack_attack, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11038/11038 [==============================] - 0s 23us/step\n",
      "53600/53600 [==============================] - 1s 21us/step\n",
      "L2 loss of Attack Packs: {'mean': 0.6795025582222834, 'std': 1.082768638995405, 'max': 4.387752791166377, 'min': 0.00045515118283426535}\n",
      "L2 Loss of Normal Packs: {'mean': 0.00299856907399672, 'std': 0.024648300730335262, 'max': 1.9500067300648791, 'min': 1.60817596375893e-05}\n"
     ]
    }
   ],
   "source": [
    "%run NSL_KDD_AE_V2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以见到，当Autoencoder训练收敛后，对原数据可以还原，而对攻击数据则不能。（或有很大的L2 loss）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验时间：2019/10/17 9:48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步进行实验，这次则是分出部分正常数据加入测试集。实验再一次验证了异常数据（或者说攻击数据）是无法被收敛的Autoencoder还原的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83625\\DataIDPS\\Autoencoder\\NSL_KDD_AE_V2.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pack_train = pack_train / pack_max\n",
      "C:\\Users\\83625\\DataIDPS\\Autoencoder\\NSL_KDD_AE_V2.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pack_test = pack_test / pack_max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 34049 samples, validate on 8513 samples\n",
      "Epoch 1/5000\n",
      "34049/34049 [==============================] - 3s 81us/step - loss: 0.0578 - val_loss: 0.0123\n",
      "Epoch 2/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0083 - val_loss: 0.0062\n",
      "Epoch 3/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 4/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 5/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 6/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 7/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 8/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 9/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 10/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 11/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 0.0010 - val_loss: 9.4025e-04\n",
      "Epoch 12/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.2104e-04 - val_loss: 8.6038e-04\n",
      "Epoch 13/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.4865e-04 - val_loss: 8.0859e-04\n",
      "Epoch 14/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 7.9705e-04 - val_loss: 7.4668e-04\n",
      "Epoch 15/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 7.5549e-04 - val_loss: 7.1226e-04\n",
      "Epoch 16/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 7.2213e-04 - val_loss: 7.1681e-04\n",
      "Epoch 17/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 6.9920e-04 - val_loss: 6.8289e-04\n",
      "Epoch 18/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 6.6636e-04 - val_loss: 6.3135e-04\n",
      "Epoch 19/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 6.3188e-04 - val_loss: 6.1351e-04\n",
      "Epoch 20/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 5.9781e-04 - val_loss: 5.9686e-04\n",
      "Epoch 21/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 5.6624e-04 - val_loss: 6.0948e-04\n",
      "Epoch 22/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 5.3064e-04 - val_loss: 5.1215e-04\n",
      "Epoch 23/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 5.0244e-04 - val_loss: 5.1747e-04\n",
      "Epoch 24/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 4.8585e-04 - val_loss: 4.9003e-04\n",
      "Epoch 25/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 4.6714e-04 - val_loss: 4.6014e-04\n",
      "Epoch 26/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 4.5303e-04 - val_loss: 5.1557e-04\n",
      "Epoch 27/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 4.6154e-04 - val_loss: 4.5010e-04\n",
      "Epoch 28/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 4.2899e-04 - val_loss: 4.2986e-04\n",
      "Epoch 29/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 4.1778e-04 - val_loss: 4.1443e-04\n",
      "Epoch 30/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 4.0639e-04 - val_loss: 4.5115e-04\n",
      "Epoch 31/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 3.9780e-04 - val_loss: 4.1560e-04\n",
      "Epoch 32/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 3.8662e-04 - val_loss: 3.9212e-04\n",
      "Epoch 33/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 3.8133e-04 - val_loss: 4.7924e-04\n",
      "Epoch 34/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 4.2860e-04 - val_loss: 3.7578e-04\n",
      "Epoch 35/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.6535e-04 - val_loss: 3.7824e-04\n",
      "Epoch 36/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 3.5767e-04 - val_loss: 3.9324e-04\n",
      "Epoch 37/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 3.5309e-04 - val_loss: 4.0507e-04\n",
      "Epoch 38/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.4871e-04 - val_loss: 3.9571e-04\n",
      "Epoch 39/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 3.3850e-04 - val_loss: 4.4350e-04\n",
      "Epoch 40/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.4824e-04 - val_loss: 3.5215e-04\n",
      "Epoch 41/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 3.2642e-04 - val_loss: 3.5007e-04\n",
      "Epoch 42/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.2291e-04 - val_loss: 3.7582e-04\n",
      "Epoch 43/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.2559e-04 - val_loss: 3.3429e-04\n",
      "Epoch 44/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 3.1169e-04 - val_loss: 4.2437e-04\n",
      "Epoch 45/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.1569e-04 - val_loss: 3.5629e-04\n",
      "Epoch 46/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 3.0158e-04 - val_loss: 3.1547e-04\n",
      "Epoch 47/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.9802e-04 - val_loss: 3.3064e-04\n",
      "Epoch 48/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.9472e-04 - val_loss: 3.3854e-04\n",
      "Epoch 49/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.9248e-04 - val_loss: 3.6023e-04\n",
      "Epoch 50/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.9042e-04 - val_loss: 3.9211e-04\n",
      "Epoch 51/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.9014e-04 - val_loss: 3.4080e-04\n",
      "Epoch 52/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.8459e-04 - val_loss: 3.1093e-04\n",
      "Epoch 53/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.7960e-04 - val_loss: 3.0472e-04\n",
      "Epoch 54/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.7538e-04 - val_loss: 3.3608e-04\n",
      "Epoch 55/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.7809e-04 - val_loss: 3.5895e-04\n",
      "Epoch 56/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.7619e-04 - val_loss: 2.9788e-04\n",
      "Epoch 57/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.7043e-04 - val_loss: 2.8354e-04\n",
      "Epoch 58/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.6488e-04 - val_loss: 2.9627e-04\n",
      "Epoch 59/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.6252e-04 - val_loss: 2.8065e-04\n",
      "Epoch 60/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.6004e-04 - val_loss: 2.7119e-04\n",
      "Epoch 61/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.5791e-04 - val_loss: 3.9630e-04\n",
      "Epoch 62/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.6133e-04 - val_loss: 3.2437e-04\n",
      "Epoch 63/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.5374e-04 - val_loss: 2.6836e-04\n",
      "Epoch 64/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 2.4753e-04 - val_loss: 2.6003e-04\n",
      "Epoch 65/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 2.4585e-04 - val_loss: 3.5132e-04\n",
      "Epoch 66/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.5219e-04 - val_loss: 2.5986e-04\n",
      "Epoch 67/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.4031e-04 - val_loss: 2.8489e-04\n",
      "Epoch 68/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.3812e-04 - val_loss: 2.5779e-04\n",
      "Epoch 69/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.3574e-04 - val_loss: 2.9021e-04\n",
      "Epoch 70/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.3389e-04 - val_loss: 2.8961e-04\n",
      "Epoch 71/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.3380e-04 - val_loss: 2.4908e-04\n",
      "Epoch 72/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.2830e-04 - val_loss: 2.5370e-04\n",
      "Epoch 73/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.2775e-04 - val_loss: 2.6438e-04\n",
      "Epoch 74/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.2600e-04 - val_loss: 2.3694e-04\n",
      "Epoch 75/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.2476e-04 - val_loss: 2.7236e-04\n",
      "Epoch 76/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.2370e-04 - val_loss: 2.8150e-04\n",
      "Epoch 77/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.2403e-04 - val_loss: 2.3553e-04\n",
      "Epoch 78/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 2.1683e-04 - val_loss: 2.3443e-04\n",
      "Epoch 79/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.1399e-04 - val_loss: 2.4042e-04\n",
      "Epoch 80/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.1167e-04 - val_loss: 2.4408e-04\n",
      "Epoch 81/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.1060e-04 - val_loss: 2.6941e-04\n",
      "Epoch 82/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.1428e-04 - val_loss: 2.3951e-04\n",
      "Epoch 83/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.0597e-04 - val_loss: 2.8214e-04\n",
      "Epoch 84/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.0593e-04 - val_loss: 2.7812e-04\n",
      "Epoch 85/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.0655e-04 - val_loss: 2.2342e-04\n",
      "Epoch 86/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 2.0059e-04 - val_loss: 2.1866e-04\n",
      "Epoch 87/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9848e-04 - val_loss: 2.8048e-04\n",
      "Epoch 88/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.0474e-04 - val_loss: 2.1368e-04\n",
      "Epoch 89/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9682e-04 - val_loss: 2.6328e-04\n",
      "Epoch 90/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9645e-04 - val_loss: 2.1426e-04\n",
      "Epoch 91/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.9255e-04 - val_loss: 2.0841e-04\n",
      "Epoch 92/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9219e-04 - val_loss: 2.6234e-04\n",
      "Epoch 93/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.9571e-04 - val_loss: 2.2215e-04\n",
      "Epoch 94/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.9062e-04 - val_loss: 3.0664e-04\n",
      "Epoch 95/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 2.1513e-04 - val_loss: 2.1963e-04\n",
      "Epoch 96/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9584e-04 - val_loss: 2.1217e-04\n",
      "Epoch 97/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8574e-04 - val_loss: 2.6083e-04\n",
      "Epoch 98/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9382e-04 - val_loss: 2.8125e-04\n",
      "Epoch 99/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.8945e-04 - val_loss: 2.0594e-04\n",
      "Epoch 100/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 1.8452e-04 - val_loss: 2.9473e-04\n",
      "Epoch 101/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8844e-04 - val_loss: 2.1601e-04\n",
      "Epoch 102/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.8336e-04 - val_loss: 2.6252e-04\n",
      "Epoch 103/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.9034e-04 - val_loss: 2.0711e-04\n",
      "Epoch 104/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8189e-04 - val_loss: 2.0998e-04\n",
      "Epoch 105/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8271e-04 - val_loss: 2.0977e-04\n",
      "Epoch 106/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.8162e-04 - val_loss: 2.0280e-04\n",
      "Epoch 107/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8090e-04 - val_loss: 2.0014e-04\n",
      "Epoch 108/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.8010e-04 - val_loss: 2.0455e-04\n",
      "Epoch 109/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7932e-04 - val_loss: 1.9967e-04\n",
      "Epoch 110/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7892e-04 - val_loss: 2.8034e-04\n",
      "Epoch 111/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 2.2473e-04 - val_loss: 2.3180e-04\n",
      "Epoch 112/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.8252e-04 - val_loss: 1.9872e-04\n",
      "Epoch 113/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7720e-04 - val_loss: 2.4607e-04\n",
      "Epoch 114/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7894e-04 - val_loss: 2.0946e-04\n",
      "Epoch 115/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.7749e-04 - val_loss: 2.3488e-04\n",
      "Epoch 116/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 1.7934e-04 - val_loss: 2.3352e-04\n",
      "Epoch 117/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7860e-04 - val_loss: 1.9720e-04\n",
      "Epoch 118/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.7468e-04 - val_loss: 2.8996e-04\n",
      "Epoch 119/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.7960e-04 - val_loss: 2.1711e-04\n",
      "Epoch 120/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7433e-04 - val_loss: 2.2487e-04\n",
      "Epoch 121/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7487e-04 - val_loss: 2.2022e-04\n",
      "Epoch 122/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7358e-04 - val_loss: 1.9176e-04\n",
      "Epoch 123/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.7247e-04 - val_loss: 2.7408e-04\n",
      "Epoch 124/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.7429e-04 - val_loss: 1.9193e-04\n",
      "Epoch 125/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 1.7144e-04 - val_loss: 2.1223e-04\n",
      "Epoch 126/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.7374e-04 - val_loss: 2.1496e-04\n",
      "Epoch 127/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.7117e-04 - val_loss: 3.1119e-04\n",
      "Epoch 128/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7859e-04 - val_loss: 2.4117e-04\n",
      "Epoch 129/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.7189e-04 - val_loss: 1.9652e-04\n",
      "Epoch 130/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.6973e-04 - val_loss: 1.9324e-04\n",
      "Epoch 131/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.6815e-04 - val_loss: 2.2314e-04\n",
      "Epoch 132/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.6806e-04 - val_loss: 2.4940e-04\n",
      "Epoch 133/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7406e-04 - val_loss: 2.0173e-04\n",
      "Epoch 134/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6825e-04 - val_loss: 1.9460e-04\n",
      "Epoch 135/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.6781e-04 - val_loss: 2.0687e-04\n",
      "Epoch 136/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6682e-04 - val_loss: 2.1085e-04\n",
      "Epoch 137/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.6860e-04 - val_loss: 1.9485e-04\n",
      "Epoch 138/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6780e-04 - val_loss: 2.7214e-04\n",
      "Epoch 139/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 1.8400e-04 - val_loss: 2.2351e-04\n",
      "Epoch 140/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.6701e-04 - val_loss: 2.7982e-04\n",
      "Epoch 141/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6782e-04 - val_loss: 1.8805e-04\n",
      "Epoch 142/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6225e-04 - val_loss: 2.0807e-04\n",
      "Epoch 143/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6626e-04 - val_loss: 1.9570e-04\n",
      "Epoch 144/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6320e-04 - val_loss: 1.8919e-04\n",
      "Epoch 145/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6156e-04 - val_loss: 1.8914e-04\n",
      "Epoch 146/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6235e-04 - val_loss: 2.0876e-04\n",
      "Epoch 147/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6817e-04 - val_loss: 1.8263e-04\n",
      "Epoch 148/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.6050e-04 - val_loss: 1.8864e-04\n",
      "Epoch 149/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6126e-04 - val_loss: 1.8686e-04\n",
      "Epoch 150/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5977e-04 - val_loss: 1.8055e-04\n",
      "Epoch 151/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.5984e-04 - val_loss: 2.7097e-04\n",
      "Epoch 152/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6382e-04 - val_loss: 2.0569e-04\n",
      "Epoch 153/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6014e-04 - val_loss: 2.1598e-04\n",
      "Epoch 154/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6189e-04 - val_loss: 2.2183e-04\n",
      "Epoch 155/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6468e-04 - val_loss: 1.8685e-04\n",
      "Epoch 156/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5998e-04 - val_loss: 2.2446e-04\n",
      "Epoch 157/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6025e-04 - val_loss: 2.1200e-04\n",
      "Epoch 158/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6113e-04 - val_loss: 1.8253e-04\n",
      "Epoch 159/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.5601e-04 - val_loss: 1.8809e-04\n",
      "Epoch 160/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5511e-04 - val_loss: 1.8089e-04\n",
      "Epoch 161/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.5584e-04 - val_loss: 1.9873e-04\n",
      "Epoch 162/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5509e-04 - val_loss: 1.9847e-04\n",
      "Epoch 163/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5733e-04 - val_loss: 2.5103e-04\n",
      "Epoch 164/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.6487e-04 - val_loss: 1.7556e-04\n",
      "Epoch 165/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5377e-04 - val_loss: 2.0492e-04\n",
      "Epoch 166/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.5783e-04 - val_loss: 1.8001e-04\n",
      "Epoch 167/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5310e-04 - val_loss: 1.8417e-04\n",
      "Epoch 168/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5264e-04 - val_loss: 1.9032e-04\n",
      "Epoch 169/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.5258e-04 - val_loss: 1.8710e-04\n",
      "Epoch 170/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5416e-04 - val_loss: 1.7757e-04\n",
      "Epoch 171/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.5169e-04 - val_loss: 1.8465e-04\n",
      "Epoch 172/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5175e-04 - val_loss: 2.7346e-04\n",
      "Epoch 173/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6740e-04 - val_loss: 2.2644e-04\n",
      "Epoch 174/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 1.5266e-04 - val_loss: 1.7523e-04\n",
      "Epoch 175/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.5079e-04 - val_loss: 1.7812e-04\n",
      "Epoch 176/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4963e-04 - val_loss: 2.1538e-04\n",
      "Epoch 177/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5372e-04 - val_loss: 1.8264e-04\n",
      "Epoch 178/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.4973e-04 - val_loss: 2.0096e-04\n",
      "Epoch 179/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.5116e-04 - val_loss: 1.8384e-04\n",
      "Epoch 180/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4946e-04 - val_loss: 3.0807e-04\n",
      "Epoch 181/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6844e-04 - val_loss: 1.7621e-04\n",
      "Epoch 182/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4757e-04 - val_loss: 1.7256e-04\n",
      "Epoch 183/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.4642e-04 - val_loss: 1.8158e-04\n",
      "Epoch 184/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4763e-04 - val_loss: 1.7126e-04\n",
      "Epoch 185/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4659e-04 - val_loss: 1.8305e-04\n",
      "Epoch 186/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4945e-04 - val_loss: 1.7609e-04\n",
      "Epoch 187/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4673e-04 - val_loss: 2.3583e-04\n",
      "Epoch 188/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.7740e-04 - val_loss: 1.7586e-04\n",
      "Epoch 189/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4503e-04 - val_loss: 2.0537e-04\n",
      "Epoch 190/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4654e-04 - val_loss: 1.7615e-04\n",
      "Epoch 191/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4505e-04 - val_loss: 1.7671e-04\n",
      "Epoch 192/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4505e-04 - val_loss: 1.8362e-04\n",
      "Epoch 193/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4541e-04 - val_loss: 1.9179e-04\n",
      "Epoch 194/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4765e-04 - val_loss: 2.0341e-04\n",
      "Epoch 195/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4530e-04 - val_loss: 2.5097e-04\n",
      "Epoch 196/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4765e-04 - val_loss: 2.0290e-04\n",
      "Epoch 197/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4927e-04 - val_loss: 1.6877e-04\n",
      "Epoch 198/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.4303e-04 - val_loss: 1.6775e-04\n",
      "Epoch 199/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 1.4338e-04 - val_loss: 2.0371e-04\n",
      "Epoch 200/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 1.4556e-04 - val_loss: 2.0202e-04\n",
      "Epoch 201/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4525e-04 - val_loss: 1.8224e-04\n",
      "Epoch 202/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4498e-04 - val_loss: 2.0788e-04\n",
      "Epoch 203/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4424e-04 - val_loss: 3.1448e-04\n",
      "Epoch 204/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.6349e-04 - val_loss: 2.4962e-04\n",
      "Epoch 205/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4477e-04 - val_loss: 1.7578e-04\n",
      "Epoch 206/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4214e-04 - val_loss: 1.7881e-04\n",
      "Epoch 207/5000\n",
      "34049/34049 [==============================] - ETA: 0s - loss: 1.4052e-0 - 1s 32us/step - loss: 1.4059e-04 - val_loss: 1.9773e-04\n",
      "Epoch 208/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4187e-04 - val_loss: 1.6632e-04\n",
      "Epoch 209/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4067e-04 - val_loss: 2.2395e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.5562e-04 - val_loss: 2.2314e-04\n",
      "Epoch 211/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4475e-04 - val_loss: 1.6387e-04\n",
      "Epoch 212/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3962e-04 - val_loss: 1.9076e-04\n",
      "Epoch 213/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4240e-04 - val_loss: 1.7767e-04\n",
      "Epoch 214/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4026e-04 - val_loss: 1.8843e-04\n",
      "Epoch 215/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3934e-04 - val_loss: 1.6351e-04\n",
      "Epoch 216/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3792e-04 - val_loss: 1.7479e-04\n",
      "Epoch 217/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3900e-04 - val_loss: 1.8651e-04\n",
      "Epoch 218/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4019e-04 - val_loss: 1.7620e-04\n",
      "Epoch 219/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3838e-04 - val_loss: 1.8002e-04\n",
      "Epoch 220/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3818e-04 - val_loss: 1.9688e-04\n",
      "Epoch 221/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 1.3922e-04 - val_loss: 2.0097e-04\n",
      "Epoch 222/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4047e-04 - val_loss: 2.3530e-04\n",
      "Epoch 223/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4653e-04 - val_loss: 1.6420e-04\n",
      "Epoch 224/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3609e-04 - val_loss: 1.6310e-04\n",
      "Epoch 225/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3609e-04 - val_loss: 1.7242e-04\n",
      "Epoch 226/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3678e-04 - val_loss: 1.6530e-04\n",
      "Epoch 227/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3518e-04 - val_loss: 1.6661e-04\n",
      "Epoch 228/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.3657e-04 - val_loss: 1.8024e-04\n",
      "Epoch 229/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.3853e-04 - val_loss: 2.6153e-04\n",
      "Epoch 230/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4001e-04 - val_loss: 1.7468e-04\n",
      "Epoch 231/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3446e-04 - val_loss: 1.8527e-04\n",
      "Epoch 232/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3595e-04 - val_loss: 1.7821e-04\n",
      "Epoch 233/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3356e-04 - val_loss: 1.6412e-04\n",
      "Epoch 234/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3415e-04 - val_loss: 1.9324e-04\n",
      "Epoch 235/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3502e-04 - val_loss: 2.1560e-04\n",
      "Epoch 236/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3761e-04 - val_loss: 1.6647e-04\n",
      "Epoch 237/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3382e-04 - val_loss: 1.6922e-04\n",
      "Epoch 238/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3193e-04 - val_loss: 2.1642e-04\n",
      "Epoch 239/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3973e-04 - val_loss: 2.0570e-04\n",
      "Epoch 240/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.4187e-04 - val_loss: 1.6418e-04\n",
      "Epoch 241/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2950e-04 - val_loss: 1.7962e-04\n",
      "Epoch 242/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3247e-04 - val_loss: 1.6169e-04\n",
      "Epoch 243/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.2892e-04 - val_loss: 1.6835e-04\n",
      "Epoch 244/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3184e-04 - val_loss: 1.9260e-04\n",
      "Epoch 245/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.3739e-04 - val_loss: 1.6549e-04\n",
      "Epoch 246/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.2837e-04 - val_loss: 1.7222e-04\n",
      "Epoch 247/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2841e-04 - val_loss: 1.5280e-04\n",
      "Epoch 248/5000\n",
      "34049/34049 [==============================] - 1s 36us/step - loss: 1.2727e-04 - val_loss: 1.7198e-04\n",
      "Epoch 249/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2740e-04 - val_loss: 2.2475e-04\n",
      "Epoch 250/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 1.3333e-04 - val_loss: 1.8104e-04\n",
      "Epoch 251/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2833e-04 - val_loss: 1.5471e-04\n",
      "Epoch 252/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2560e-04 - val_loss: 1.5488e-04\n",
      "Epoch 253/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2511e-04 - val_loss: 1.6162e-04\n",
      "Epoch 254/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.2590e-04 - val_loss: 1.7614e-04\n",
      "Epoch 255/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2579e-04 - val_loss: 1.5816e-04\n",
      "Epoch 256/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2451e-04 - val_loss: 1.5443e-04\n",
      "Epoch 257/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2396e-04 - val_loss: 2.2163e-04\n",
      "Epoch 258/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2751e-04 - val_loss: 1.8771e-04\n",
      "Epoch 259/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2431e-04 - val_loss: 1.5721e-04\n",
      "Epoch 260/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2237e-04 - val_loss: 1.5713e-04\n",
      "Epoch 261/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2373e-04 - val_loss: 1.5092e-04\n",
      "Epoch 262/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.2060e-04 - val_loss: 1.6058e-04\n",
      "Epoch 263/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.2231e-04 - val_loss: 1.5609e-04\n",
      "Epoch 264/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2063e-04 - val_loss: 1.6739e-04\n",
      "Epoch 265/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2126e-04 - val_loss: 1.8088e-04\n",
      "Epoch 266/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2144e-04 - val_loss: 1.5598e-04\n",
      "Epoch 267/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.1774e-04 - val_loss: 1.5210e-04\n",
      "Epoch 268/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1799e-04 - val_loss: 1.5252e-04\n",
      "Epoch 269/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1752e-04 - val_loss: 1.4949e-04\n",
      "Epoch 270/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1634e-04 - val_loss: 1.5077e-04\n",
      "Epoch 271/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.1649e-04 - val_loss: 1.4408e-04\n",
      "Epoch 272/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.1509e-04 - val_loss: 1.6586e-04\n",
      "Epoch 273/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1656e-04 - val_loss: 2.0760e-04\n",
      "Epoch 274/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1901e-04 - val_loss: 1.7214e-04\n",
      "Epoch 275/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1659e-04 - val_loss: 1.4965e-04\n",
      "Epoch 276/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1550e-04 - val_loss: 1.7659e-04\n",
      "Epoch 277/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1605e-04 - val_loss: 1.4711e-04\n",
      "Epoch 278/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1311e-04 - val_loss: 1.4720e-04\n",
      "Epoch 279/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.1270e-04 - val_loss: 1.6983e-04\n",
      "Epoch 280/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1535e-04 - val_loss: 1.4003e-04\n",
      "Epoch 281/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.1212e-04 - val_loss: 1.4570e-04\n",
      "Epoch 282/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1112e-04 - val_loss: 1.3897e-04\n",
      "Epoch 283/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1071e-04 - val_loss: 1.5560e-04\n",
      "Epoch 284/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1013e-04 - val_loss: 1.4764e-04\n",
      "Epoch 285/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1075e-04 - val_loss: 1.4559e-04\n",
      "Epoch 286/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0930e-04 - val_loss: 1.3788e-04\n",
      "Epoch 287/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.0919e-04 - val_loss: 1.3994e-04\n",
      "Epoch 288/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0762e-04 - val_loss: 1.4336e-04\n",
      "Epoch 289/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0811e-04 - val_loss: 1.4625e-04\n",
      "Epoch 290/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0955e-04 - val_loss: 1.6802e-04\n",
      "Epoch 291/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1080e-04 - val_loss: 1.5086e-04\n",
      "Epoch 292/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0932e-04 - val_loss: 1.4273e-04\n",
      "Epoch 293/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0499e-04 - val_loss: 1.5771e-04\n",
      "Epoch 294/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0442e-04 - val_loss: 1.5724e-04\n",
      "Epoch 295/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0594e-04 - val_loss: 1.6597e-04\n",
      "Epoch 296/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0572e-04 - val_loss: 1.4506e-04\n",
      "Epoch 297/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0553e-04 - val_loss: 1.9414e-04\n",
      "Epoch 298/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0619e-04 - val_loss: 1.5316e-04\n",
      "Epoch 299/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0454e-04 - val_loss: 1.3726e-04\n",
      "Epoch 300/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.0283e-04 - val_loss: 1.7346e-04\n",
      "Epoch 301/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0549e-04 - val_loss: 1.4231e-04\n",
      "Epoch 302/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0337e-04 - val_loss: 1.3876e-04\n",
      "Epoch 303/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0267e-04 - val_loss: 1.3366e-04\n",
      "Epoch 304/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0190e-04 - val_loss: 1.4246e-04\n",
      "Epoch 305/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0154e-04 - val_loss: 1.3932e-04\n",
      "Epoch 306/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0164e-04 - val_loss: 1.5295e-04\n",
      "Epoch 307/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0342e-04 - val_loss: 1.8478e-04\n",
      "Epoch 308/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0429e-04 - val_loss: 1.4508e-04\n",
      "Epoch 309/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0150e-04 - val_loss: 1.4835e-04\n",
      "Epoch 310/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0083e-04 - val_loss: 1.3527e-04\n",
      "Epoch 311/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.9509e-05 - val_loss: 1.4182e-04\n",
      "Epoch 312/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.9541e-05 - val_loss: 1.3557e-04\n",
      "Epoch 313/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.9297e-05 - val_loss: 1.3643e-04\n",
      "Epoch 314/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.9827e-05 - val_loss: 1.3716e-04\n",
      "Epoch 315/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0148e-04 - val_loss: 1.6848e-04\n",
      "Epoch 316/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0195e-04 - val_loss: 1.3249e-04\n",
      "Epoch 317/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.8478e-05 - val_loss: 1.2955e-04\n",
      "Epoch 318/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.9884e-05 - val_loss: 1.3689e-04\n",
      "Epoch 319/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.8366e-05 - val_loss: 1.4210e-04\n",
      "Epoch 320/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.7640e-05 - val_loss: 2.0519e-04\n",
      "Epoch 321/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.2911e-04 - val_loss: 1.5466e-04\n",
      "Epoch 322/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.8845e-05 - val_loss: 1.4480e-04\n",
      "Epoch 323/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.7814e-05 - val_loss: 1.4319e-04\n",
      "Epoch 324/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.8029e-05 - val_loss: 1.3639e-04\n",
      "Epoch 325/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.7399e-05 - val_loss: 1.3387e-04\n",
      "Epoch 326/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.6998e-05 - val_loss: 1.2888e-04\n",
      "Epoch 327/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.6645e-05 - val_loss: 1.3733e-04\n",
      "Epoch 328/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.7056e-05 - val_loss: 1.3077e-04\n",
      "Epoch 329/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.6321e-05 - val_loss: 1.2975e-04\n",
      "Epoch 330/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.6396e-05 - val_loss: 1.3024e-04\n",
      "Epoch 331/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.6062e-05 - val_loss: 2.2135e-04\n",
      "Epoch 332/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0510e-04 - val_loss: 1.2986e-04\n",
      "Epoch 333/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.6069e-05 - val_loss: 1.5837e-04\n",
      "Epoch 334/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.6369e-05 - val_loss: 1.3917e-04\n",
      "Epoch 335/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5740e-05 - val_loss: 1.3886e-04\n",
      "Epoch 336/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.6068e-05 - val_loss: 1.2897e-04\n",
      "Epoch 337/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.4586e-05 - val_loss: 1.3579e-04\n",
      "Epoch 338/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.7217e-05 - val_loss: 1.5996e-04\n",
      "Epoch 339/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.7431e-05 - val_loss: 1.3033e-04\n",
      "Epoch 340/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.7968e-05 - val_loss: 1.3176e-04\n",
      "Epoch 341/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5420e-05 - val_loss: 1.2989e-04\n",
      "Epoch 342/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4632e-05 - val_loss: 1.5777e-04\n",
      "Epoch 343/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.5823e-05 - val_loss: 1.4230e-04\n",
      "Epoch 344/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.6388e-05 - val_loss: 1.3303e-04\n",
      "Epoch 345/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.4496e-05 - val_loss: 1.3194e-04\n",
      "Epoch 346/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5177e-05 - val_loss: 1.2836e-04\n",
      "Epoch 347/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5054e-05 - val_loss: 1.3209e-04\n",
      "Epoch 348/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4794e-05 - val_loss: 2.1163e-04\n",
      "Epoch 349/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0713e-04 - val_loss: 1.4732e-04\n",
      "Epoch 350/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4633e-05 - val_loss: 1.7223e-04\n",
      "Epoch 351/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 1.0594e-04 - val_loss: 1.5920e-04\n",
      "Epoch 352/5000\n",
      "34049/34049 [==============================] - 1s 30us/step - loss: 9.5429e-05 - val_loss: 1.2950e-04\n",
      "Epoch 353/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.3698e-05 - val_loss: 1.4726e-04\n",
      "Epoch 354/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.4190e-05 - val_loss: 1.2662e-04\n",
      "Epoch 355/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.2956e-05 - val_loss: 1.6313e-04\n",
      "Epoch 356/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.6506e-05 - val_loss: 1.3166e-04\n",
      "Epoch 357/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3159e-05 - val_loss: 1.8558e-04\n",
      "Epoch 358/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.6041e-05 - val_loss: 1.2638e-04\n",
      "Epoch 359/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.2825e-05 - val_loss: 1.2793e-04\n",
      "Epoch 360/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3453e-05 - val_loss: 1.7476e-04\n",
      "Epoch 361/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0225e-04 - val_loss: 1.3672e-04\n",
      "Epoch 362/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3967e-05 - val_loss: 1.4734e-04\n",
      "Epoch 363/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4416e-05 - val_loss: 1.3156e-04\n",
      "Epoch 364/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2941e-05 - val_loss: 1.5489e-04\n",
      "Epoch 365/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0335e-04 - val_loss: 1.2894e-04\n",
      "Epoch 366/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2423e-05 - val_loss: 1.2785e-04\n",
      "Epoch 367/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2534e-05 - val_loss: 2.1925e-04\n",
      "Epoch 368/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0233e-04 - val_loss: 1.7700e-04\n",
      "Epoch 369/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4698e-05 - val_loss: 1.5381e-04\n",
      "Epoch 370/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3571e-05 - val_loss: 1.3277e-04\n",
      "Epoch 371/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2625e-05 - val_loss: 1.3768e-04\n",
      "Epoch 372/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2663e-05 - val_loss: 1.5391e-04\n",
      "Epoch 373/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3330e-05 - val_loss: 1.3876e-04\n",
      "Epoch 374/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3553e-05 - val_loss: 1.4002e-04\n",
      "Epoch 375/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4885e-05 - val_loss: 1.6502e-04\n",
      "Epoch 376/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.6100e-05 - val_loss: 1.2309e-04\n",
      "Epoch 377/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2053e-05 - val_loss: 1.3046e-04\n",
      "Epoch 378/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3271e-05 - val_loss: 1.8770e-04\n",
      "Epoch 379/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.4235e-05 - val_loss: 1.3567e-04\n",
      "Epoch 380/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2129e-05 - val_loss: 1.2744e-04\n",
      "Epoch 381/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1636e-05 - val_loss: 2.2601e-04\n",
      "Epoch 382/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.1033e-04 - val_loss: 1.2627e-04\n",
      "Epoch 383/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1840e-05 - val_loss: 1.5120e-04\n",
      "Epoch 384/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.2763e-05 - val_loss: 1.2765e-04\n",
      "Epoch 385/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.2218e-05 - val_loss: 1.4358e-04\n",
      "Epoch 386/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1339e-05 - val_loss: 1.4703e-04\n",
      "Epoch 387/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.8358e-05 - val_loss: 2.1044e-04\n",
      "Epoch 388/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0888e-04 - val_loss: 1.3747e-04\n",
      "Epoch 389/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.1739e-05 - val_loss: 2.3027e-04\n",
      "Epoch 390/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.9117e-05 - val_loss: 1.2687e-04\n",
      "Epoch 391/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0408e-05 - val_loss: 1.3017e-04\n",
      "Epoch 392/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1051e-05 - val_loss: 2.8068e-04\n",
      "Epoch 393/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.4427e-04 - val_loss: 2.2460e-04\n",
      "Epoch 394/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.0611e-04 - val_loss: 1.2723e-04\n",
      "Epoch 395/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9908e-05 - val_loss: 1.2872e-04\n",
      "Epoch 396/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 8.9870e-05 - val_loss: 1.3031e-04\n",
      "Epoch 397/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 8.9932e-05 - val_loss: 1.2394e-04\n",
      "Epoch 398/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0156e-05 - val_loss: 1.2532e-04\n",
      "Epoch 399/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.1575e-05 - val_loss: 1.2225e-04\n",
      "Epoch 400/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.9751e-05 - val_loss: 1.6550e-04\n",
      "Epoch 401/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 1.0461e-04 - val_loss: 1.4427e-04\n",
      "Epoch 402/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0713e-05 - val_loss: 1.2778e-04\n",
      "Epoch 403/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9898e-05 - val_loss: 1.2559e-04\n",
      "Epoch 404/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9944e-05 - val_loss: 1.2267e-04\n",
      "Epoch 405/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0264e-05 - val_loss: 1.2996e-04\n",
      "Epoch 406/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1147e-05 - val_loss: 1.4766e-04\n",
      "Epoch 407/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3487e-05 - val_loss: 1.2215e-04\n",
      "Epoch 408/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0467e-05 - val_loss: 1.2520e-04\n",
      "Epoch 409/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0249e-05 - val_loss: 1.3631e-04\n",
      "Epoch 410/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1676e-05 - val_loss: 2.4602e-04\n",
      "Epoch 411/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5070e-05 - val_loss: 1.2379e-04\n",
      "Epoch 412/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0536e-05 - val_loss: 1.5873e-04\n",
      "Epoch 413/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.5537e-05 - val_loss: 1.2820e-04\n",
      "Epoch 414/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1327e-05 - val_loss: 1.2790e-04\n",
      "Epoch 415/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9456e-05 - val_loss: 1.6126e-04\n",
      "Epoch 416/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0824e-05 - val_loss: 1.2718e-04\n",
      "Epoch 417/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9776e-05 - val_loss: 1.5764e-04\n",
      "Epoch 418/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.3170e-05 - val_loss: 1.3291e-04\n",
      "Epoch 419/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0307e-05 - val_loss: 1.3773e-04\n",
      "Epoch 420/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0205e-05 - val_loss: 1.4150e-04\n",
      "Epoch 421/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1674e-05 - val_loss: 1.2684e-04\n",
      "Epoch 422/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 8.9891e-05 - val_loss: 1.3855e-04\n",
      "Epoch 423/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 9.0576e-05 - val_loss: 1.3422e-04\n",
      "Epoch 424/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.0034e-05 - val_loss: 1.2415e-04\n",
      "Epoch 425/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 9.0132e-05 - val_loss: 1.3952e-04\n",
      "Epoch 426/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0744e-05 - val_loss: 1.4240e-04\n",
      "Epoch 427/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.2022e-05 - val_loss: 1.3642e-04\n",
      "Epoch 428/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.0229e-05 - val_loss: 1.3316e-04\n",
      "Epoch 429/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9308e-05 - val_loss: 1.3208e-04\n",
      "Epoch 430/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.8894e-05 - val_loss: 1.4320e-04\n",
      "Epoch 431/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 8.9625e-05 - val_loss: 1.2692e-04\n",
      "Epoch 432/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9527e-05 - val_loss: 1.2842e-04\n",
      "Epoch 433/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.9867e-05 - val_loss: 1.5443e-04\n",
      "Epoch 434/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.0119e-05 - val_loss: 1.2935e-04\n",
      "Epoch 435/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.9365e-05 - val_loss: 1.3892e-04\n",
      "Epoch 436/5000\n",
      "34049/34049 [==============================] - 1s 36us/step - loss: 8.9119e-05 - val_loss: 1.6733e-04\n",
      "Epoch 437/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.1667e-05 - val_loss: 1.6108e-04\n",
      "Epoch 438/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 9.1994e-05 - val_loss: 1.2718e-04\n",
      "Epoch 439/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9277e-05 - val_loss: 1.4064e-04\n",
      "Epoch 440/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.9310e-05 - val_loss: 1.3524e-04\n",
      "Epoch 441/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 8.9312e-05 - val_loss: 1.3311e-04\n",
      "Epoch 442/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9005e-05 - val_loss: 1.7509e-04\n",
      "Epoch 443/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 9.0789e-05 - val_loss: 1.3587e-04\n",
      "Epoch 444/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9084e-05 - val_loss: 1.3023e-04\n",
      "Epoch 445/5000\n",
      "34049/34049 [==============================] - 1s 31us/step - loss: 8.8446e-05 - val_loss: 1.3070e-04\n",
      "Epoch 446/5000\n",
      "34049/34049 [==============================] - 1s 34us/step - loss: 8.8487e-05 - val_loss: 1.6023e-04\n",
      "Epoch 447/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 9.0168e-05 - val_loss: 1.2890e-04\n",
      "Epoch 448/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.9144e-05 - val_loss: 1.2625e-04\n",
      "Epoch 449/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9168e-05 - val_loss: 1.4111e-04\n",
      "Epoch 450/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.9084e-05 - val_loss: 1.3045e-04\n",
      "Epoch 451/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.8671e-05 - val_loss: 1.2818e-04\n",
      "Epoch 452/5000\n",
      "34049/34049 [==============================] - 1s 33us/step - loss: 8.8636e-05 - val_loss: 1.6001e-04\n",
      "Epoch 453/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 1.1184e-04 - val_loss: 1.2650e-04\n",
      "Epoch 454/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.8919e-05 - val_loss: 1.2712e-04\n",
      "Epoch 455/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.7868e-05 - val_loss: 1.4571e-04\n",
      "Epoch 456/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.8776e-05 - val_loss: 1.2626e-04\n",
      "Epoch 457/5000\n",
      "34049/34049 [==============================] - 1s 32us/step - loss: 8.8550e-05 - val_loss: 1.2638e-04\n",
      " 6304/11038 [================>.............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83625\\DataIDPS\\Autoencoder\\NSL_KDD_AE_V2.py:108: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pack_attack = pack_attack / pack_max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11038/11038 [==============================] - 0s 26us/step\n",
      "11038/11038 [==============================] - 0s 23us/step\n",
      "42562/42562 [==============================] - 1s 25us/step\n",
      "L2 loss of Attack Packs: {'mean': 1.1489590284583862, 'std': 1.739688601855294, 'max': 12.139912023959518, 'min': 0.0004261287830564379}\n",
      "L2 Loss of Normal Packs: {'mean': 0.005500194352387333, 'std': 0.10553573427664396, 'max': 10.669008165676068, 'min': 4.8972400195560614e-05}\n",
      "L2 Loss of Normal Packs: {'mean': 0.003696372613862235, 'std': 0.02326041032284024, 'max': 2.5765938788627953, 'min': 4.254798345064977e-05}\n"
     ]
    }
   ],
   "source": [
    "%run NSL_KDD_AE_V2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：数据分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一：阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是异常检测，所以只需要二分类。最简单的方法是求每条数据与还原数据的欧氏距离，设定阈值，将无法还原的数据（欧氏距离大的）归类为异常数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Find_threshold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝线：对攻击数据的识别率。（国六要求95%）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绿线：对正常数据的识别率。（国六要求99%）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "红线：总体准确率。（攻击数据与正常数据1:1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：全连接分类器 Fully Connected Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用神经网络构建分类器。学术界对于NSL-KDD数据集往往使用特征提取，再进行监督式学习的方法。而本实验对监督式学习的需求较低。分类器本质上是对低值数据（参数上欧氏距离低的）和高值数据进行分类。所以分类器一经训练好就不再需要更改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验时间：2019/10/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16060 samples, validate on 4016 samples\n",
      "Epoch 1/5000\n",
      "16060/16060 [==============================] - 1s 64us/step - loss: 0.0912 - acc: 0.9649 - val_loss: 0.2003 - val_acc: 0.9295\n",
      "Epoch 2/5000\n",
      "16060/16060 [==============================] - 1s 36us/step - loss: 0.0949 - acc: 0.9614 - val_loss: 0.1357 - val_acc: 0.9654\n",
      "Epoch 3/5000\n",
      "16060/16060 [==============================] - 1s 39us/step - loss: 0.0895 - acc: 0.9656 - val_loss: 0.2479 - val_acc: 0.9111\n",
      "Epoch 4/5000\n",
      "16060/16060 [==============================] - 1s 36us/step - loss: 0.0896 - acc: 0.9641 - val_loss: 0.1494 - val_acc: 0.9514\n",
      "Epoch 5/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0904 - acc: 0.9649 - val_loss: 0.1744 - val_acc: 0.9363\n",
      "Epoch 6/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0876 - acc: 0.9667 - val_loss: 0.1370 - val_acc: 0.9624\n",
      "Epoch 7/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0894 - acc: 0.9654 - val_loss: 0.1799 - val_acc: 0.9442\n",
      "Epoch 8/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0871 - acc: 0.9659 - val_loss: 0.1033 - val_acc: 0.9746\n",
      "Epoch 9/5000\n",
      "16060/16060 [==============================] - 1s 37us/step - loss: 0.0936 - acc: 0.9617 - val_loss: 0.1170 - val_acc: 0.9674\n",
      "Epoch 10/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0902 - acc: 0.9637 - val_loss: 0.2091 - val_acc: 0.9268\n",
      "Epoch 11/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0883 - acc: 0.9654 - val_loss: 0.1152 - val_acc: 0.9654\n",
      "Epoch 12/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0924 - acc: 0.9631 - val_loss: 0.1286 - val_acc: 0.9622\n",
      "Epoch 13/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0897 - acc: 0.9658 - val_loss: 0.1473 - val_acc: 0.9579\n",
      "Epoch 14/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0868 - acc: 0.9653 - val_loss: 0.1536 - val_acc: 0.9410\n",
      "Epoch 15/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0912 - acc: 0.9664 - val_loss: 0.1644 - val_acc: 0.9425\n",
      "Epoch 16/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0888 - acc: 0.9649 - val_loss: 0.1637 - val_acc: 0.9490\n",
      "Epoch 17/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0870 - acc: 0.9663 - val_loss: 0.1879 - val_acc: 0.9430\n",
      "Epoch 18/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0893 - acc: 0.9645 - val_loss: 0.1623 - val_acc: 0.9445\n",
      "Epoch 19/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0864 - acc: 0.9661 - val_loss: 0.1309 - val_acc: 0.9624\n",
      "Epoch 20/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0929 - acc: 0.9611 - val_loss: 0.1410 - val_acc: 0.9594\n",
      "Epoch 21/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0881 - acc: 0.9644 - val_loss: 0.1618 - val_acc: 0.9537\n",
      "Epoch 22/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0909 - acc: 0.9646 - val_loss: 0.1587 - val_acc: 0.9552\n",
      "Epoch 23/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0852 - acc: 0.9668 - val_loss: 0.2054 - val_acc: 0.9283\n",
      "Epoch 24/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0895 - acc: 0.9640 - val_loss: 0.1615 - val_acc: 0.9450\n",
      "Epoch 25/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0886 - acc: 0.9661 - val_loss: 0.1427 - val_acc: 0.9574\n",
      "Epoch 26/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0863 - acc: 0.9653 - val_loss: 0.1265 - val_acc: 0.9671\n",
      "Epoch 27/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0862 - acc: 0.9658 - val_loss: 0.1776 - val_acc: 0.9410\n",
      "Epoch 28/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0842 - acc: 0.9677 - val_loss: 0.2792 - val_acc: 0.9054\n",
      "Epoch 29/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0885 - acc: 0.9653 - val_loss: 0.1909 - val_acc: 0.9313\n",
      "Epoch 30/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0874 - acc: 0.9655 - val_loss: 0.1953 - val_acc: 0.9363\n",
      "Epoch 31/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0859 - acc: 0.9672 - val_loss: 0.1066 - val_acc: 0.9719\n",
      "Epoch 32/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0836 - acc: 0.9682 - val_loss: 0.1871 - val_acc: 0.9447\n",
      "Epoch 33/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0837 - acc: 0.9681 - val_loss: 0.1384 - val_acc: 0.9622\n",
      "Epoch 34/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0841 - acc: 0.9669 - val_loss: 0.1636 - val_acc: 0.9509\n",
      "Epoch 35/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0860 - acc: 0.9653 - val_loss: 0.1902 - val_acc: 0.9407\n",
      "Epoch 36/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0873 - acc: 0.9654 - val_loss: 0.1412 - val_acc: 0.9599\n",
      "Epoch 37/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0848 - acc: 0.9682 - val_loss: 0.2277 - val_acc: 0.9091\n",
      "Epoch 38/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0896 - acc: 0.9657 - val_loss: 0.1930 - val_acc: 0.9385\n",
      "Epoch 39/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0852 - acc: 0.9667 - val_loss: 0.1223 - val_acc: 0.9661\n",
      "Epoch 40/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0872 - acc: 0.9666 - val_loss: 0.1007 - val_acc: 0.9709\n",
      "Epoch 41/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0849 - acc: 0.9658 - val_loss: 0.1991 - val_acc: 0.9278\n",
      "Epoch 42/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0907 - acc: 0.9625 - val_loss: 0.2326 - val_acc: 0.9203\n",
      "Epoch 43/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0883 - acc: 0.9647 - val_loss: 0.2295 - val_acc: 0.9091\n",
      "Epoch 44/5000\n",
      "16060/16060 [==============================] - 1s 37us/step - loss: 0.0808 - acc: 0.9691 - val_loss: 0.1987 - val_acc: 0.9348\n",
      "Epoch 45/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0818 - acc: 0.9688 - val_loss: 0.1783 - val_acc: 0.9485\n",
      "Epoch 46/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0839 - acc: 0.9673 - val_loss: 0.1256 - val_acc: 0.9651\n",
      "Epoch 47/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0838 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9639\n",
      "Epoch 48/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0870 - acc: 0.9654 - val_loss: 0.3126 - val_acc: 0.8959\n",
      "Epoch 49/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0894 - acc: 0.9646 - val_loss: 0.2329 - val_acc: 0.9211\n",
      "Epoch 50/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0804 - acc: 0.9697 - val_loss: 0.1429 - val_acc: 0.9612\n",
      "Epoch 51/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0849 - acc: 0.9685 - val_loss: 0.1732 - val_acc: 0.9487\n",
      "Epoch 52/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0857 - acc: 0.9686 - val_loss: 0.1494 - val_acc: 0.9557\n",
      "Epoch 53/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0840 - acc: 0.9668 - val_loss: 0.1610 - val_acc: 0.9470\n",
      "Epoch 54/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0802 - acc: 0.9687 - val_loss: 0.1443 - val_acc: 0.9584\n",
      "Epoch 55/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0856 - acc: 0.9681 - val_loss: 0.1000 - val_acc: 0.9736\n",
      "Epoch 56/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0842 - acc: 0.9674 - val_loss: 0.1578 - val_acc: 0.9507\n",
      "Epoch 57/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0828 - acc: 0.9686 - val_loss: 0.2180 - val_acc: 0.9300\n",
      "Epoch 58/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0844 - acc: 0.9667 - val_loss: 0.1482 - val_acc: 0.9579\n",
      "Epoch 59/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0850 - acc: 0.9675 - val_loss: 0.1215 - val_acc: 0.9646\n",
      "Epoch 60/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0840 - acc: 0.9671 - val_loss: 0.1455 - val_acc: 0.9587\n",
      "Epoch 61/5000\n",
      "16060/16060 [==============================] - 1s 38us/step - loss: 0.0802 - acc: 0.9704 - val_loss: 0.1301 - val_acc: 0.9641\n",
      "Epoch 62/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0776 - acc: 0.9715 - val_loss: 0.1916 - val_acc: 0.9387\n",
      "Epoch 63/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0886 - acc: 0.9667 - val_loss: 0.1810 - val_acc: 0.9358\n",
      "Epoch 64/5000\n",
      "16060/16060 [==============================] - 1s 38us/step - loss: 0.0862 - acc: 0.9685 - val_loss: 0.1460 - val_acc: 0.9552\n",
      "Epoch 65/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0836 - acc: 0.9684 - val_loss: 0.1940 - val_acc: 0.9340\n",
      "Epoch 66/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0855 - acc: 0.9666 - val_loss: 0.2047 - val_acc: 0.9377\n",
      "Epoch 67/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0822 - acc: 0.9693 - val_loss: 0.1365 - val_acc: 0.9609\n",
      "Epoch 68/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0824 - acc: 0.9691 - val_loss: 0.1259 - val_acc: 0.9619\n",
      "Epoch 69/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0838 - acc: 0.9675 - val_loss: 0.1160 - val_acc: 0.9661\n",
      "Epoch 70/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0861 - acc: 0.9674 - val_loss: 0.1136 - val_acc: 0.9704\n",
      "Epoch 71/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0857 - acc: 0.9664 - val_loss: 0.2048 - val_acc: 0.9290\n",
      "Epoch 72/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0825 - acc: 0.9684 - val_loss: 0.1005 - val_acc: 0.9709\n",
      "Epoch 73/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0887 - acc: 0.9659 - val_loss: 0.1233 - val_acc: 0.9636\n",
      "Epoch 74/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0844 - acc: 0.9674 - val_loss: 0.1257 - val_acc: 0.9607\n",
      "Epoch 75/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0810 - acc: 0.9695 - val_loss: 0.1175 - val_acc: 0.9661\n",
      "Epoch 76/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0809 - acc: 0.9693 - val_loss: 0.1568 - val_acc: 0.9437\n",
      "Epoch 77/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0817 - acc: 0.9687 - val_loss: 0.1735 - val_acc: 0.9353\n",
      "Epoch 78/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0841 - acc: 0.9667 - val_loss: 0.1132 - val_acc: 0.9701\n",
      "Epoch 79/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0838 - acc: 0.9669 - val_loss: 0.1562 - val_acc: 0.9529\n",
      "Epoch 80/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0774 - acc: 0.9705 - val_loss: 0.1311 - val_acc: 0.9622\n",
      "Epoch 81/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0802 - acc: 0.9697 - val_loss: 0.1129 - val_acc: 0.9694\n",
      "Epoch 82/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0800 - acc: 0.9695 - val_loss: 0.1517 - val_acc: 0.9542\n",
      "Epoch 83/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0840 - acc: 0.9682 - val_loss: 0.1311 - val_acc: 0.9594\n",
      "Epoch 84/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0790 - acc: 0.9700 - val_loss: 0.1307 - val_acc: 0.9614\n",
      "Epoch 85/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0783 - acc: 0.9711 - val_loss: 0.1281 - val_acc: 0.9629\n",
      "Epoch 86/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0780 - acc: 0.9717 - val_loss: 0.2114 - val_acc: 0.9345\n",
      "Epoch 87/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0778 - acc: 0.9702 - val_loss: 0.1315 - val_acc: 0.9592\n",
      "Epoch 88/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0760 - acc: 0.9707 - val_loss: 0.1211 - val_acc: 0.9651\n",
      "Epoch 89/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0884 - acc: 0.9660 - val_loss: 0.1766 - val_acc: 0.9442\n",
      "Epoch 90/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0791 - acc: 0.9699 - val_loss: 0.1411 - val_acc: 0.9562\n",
      "Epoch 91/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0797 - acc: 0.9696 - val_loss: 0.1654 - val_acc: 0.9509\n",
      "Epoch 92/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0773 - acc: 0.9710 - val_loss: 0.1614 - val_acc: 0.9455\n",
      "Epoch 93/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0779 - acc: 0.9719 - val_loss: 0.1651 - val_acc: 0.9522\n",
      "Epoch 94/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0832 - acc: 0.9695 - val_loss: 0.1974 - val_acc: 0.9412\n",
      "Epoch 95/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0783 - acc: 0.9716 - val_loss: 0.1683 - val_acc: 0.9465\n",
      "Epoch 96/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0791 - acc: 0.9685 - val_loss: 0.1089 - val_acc: 0.9711\n",
      "Epoch 97/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0795 - acc: 0.9709 - val_loss: 0.1041 - val_acc: 0.9724\n",
      "Epoch 98/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0764 - acc: 0.9714 - val_loss: 0.1844 - val_acc: 0.9477\n",
      "Epoch 99/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0795 - acc: 0.9709 - val_loss: 0.0935 - val_acc: 0.9746\n",
      "Epoch 100/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0834 - acc: 0.9673 - val_loss: 0.2396 - val_acc: 0.9260\n",
      "Epoch 101/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0744 - acc: 0.9734 - val_loss: 0.1527 - val_acc: 0.9529\n",
      "Epoch 102/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0764 - acc: 0.9711 - val_loss: 0.2505 - val_acc: 0.9173\n",
      "Epoch 103/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0808 - acc: 0.9681 - val_loss: 0.1999 - val_acc: 0.9298\n",
      "Epoch 104/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0761 - acc: 0.9717 - val_loss: 0.1641 - val_acc: 0.9534\n",
      "Epoch 105/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0774 - acc: 0.9700 - val_loss: 0.1965 - val_acc: 0.9260\n",
      "Epoch 106/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0766 - acc: 0.9710 - val_loss: 0.1618 - val_acc: 0.9527\n",
      "Epoch 107/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0799 - acc: 0.9694 - val_loss: 0.1339 - val_acc: 0.9597\n",
      "Epoch 108/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0826 - acc: 0.9673 - val_loss: 0.1067 - val_acc: 0.9646\n",
      "Epoch 109/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0766 - acc: 0.9714 - val_loss: 0.1708 - val_acc: 0.9465\n",
      "Epoch 110/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0758 - acc: 0.9714 - val_loss: 0.2939 - val_acc: 0.9056\n",
      "Epoch 111/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0866 - acc: 0.9657 - val_loss: 0.2335 - val_acc: 0.9198\n",
      "Epoch 112/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0796 - acc: 0.9706 - val_loss: 0.1557 - val_acc: 0.9584\n",
      "Epoch 113/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0733 - acc: 0.9731 - val_loss: 0.1467 - val_acc: 0.9549\n",
      "Epoch 114/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0748 - acc: 0.9710 - val_loss: 0.1371 - val_acc: 0.9644\n",
      "Epoch 115/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0757 - acc: 0.9717 - val_loss: 0.1619 - val_acc: 0.9572\n",
      "Epoch 116/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0740 - acc: 0.9722 - val_loss: 0.2655 - val_acc: 0.9193\n",
      "Epoch 117/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060/16060 [==============================] - 0s 29us/step - loss: 0.0744 - acc: 0.9720 - val_loss: 0.2191 - val_acc: 0.9308\n",
      "Epoch 118/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0767 - acc: 0.9714 - val_loss: 0.1766 - val_acc: 0.9514\n",
      "Epoch 119/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0756 - acc: 0.9718 - val_loss: 0.1572 - val_acc: 0.9617\n",
      "Epoch 120/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0820 - acc: 0.9700 - val_loss: 0.1429 - val_acc: 0.9604\n",
      "Epoch 121/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0856 - acc: 0.9679 - val_loss: 0.1220 - val_acc: 0.9651\n",
      "Epoch 122/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0820 - acc: 0.9691 - val_loss: 0.1543 - val_acc: 0.9572\n",
      "Epoch 123/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0746 - acc: 0.9731 - val_loss: 0.2645 - val_acc: 0.9131\n",
      "Epoch 124/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0790 - acc: 0.9706 - val_loss: 0.1606 - val_acc: 0.9594\n",
      "Epoch 125/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0756 - acc: 0.9721 - val_loss: 0.1924 - val_acc: 0.9440\n",
      "Epoch 126/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0743 - acc: 0.9730 - val_loss: 0.1424 - val_acc: 0.9622\n",
      "Epoch 127/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0752 - acc: 0.9721 - val_loss: 0.1565 - val_acc: 0.9587\n",
      "Epoch 128/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0784 - acc: 0.9709 - val_loss: 0.2072 - val_acc: 0.9425\n",
      "Epoch 129/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0783 - acc: 0.9711 - val_loss: 0.1708 - val_acc: 0.9567\n",
      "Epoch 130/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0751 - acc: 0.9724 - val_loss: 0.2017 - val_acc: 0.9405\n",
      "Epoch 131/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0739 - acc: 0.9733 - val_loss: 0.1782 - val_acc: 0.9564\n",
      "Epoch 132/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0706 - acc: 0.9752 - val_loss: 0.1644 - val_acc: 0.9594\n",
      "Epoch 133/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0804 - acc: 0.9702 - val_loss: 0.1844 - val_acc: 0.9529\n",
      "Epoch 134/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0750 - acc: 0.9724 - val_loss: 0.2028 - val_acc: 0.9477\n",
      "Epoch 135/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0755 - acc: 0.9732 - val_loss: 0.1946 - val_acc: 0.9497\n",
      "Epoch 136/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0745 - acc: 0.9720 - val_loss: 0.1552 - val_acc: 0.9622\n",
      "Epoch 137/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0754 - acc: 0.9725 - val_loss: 0.2042 - val_acc: 0.9480\n",
      "Epoch 138/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0756 - acc: 0.9727 - val_loss: 0.1487 - val_acc: 0.9651\n",
      "Epoch 139/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0728 - acc: 0.9738 - val_loss: 0.1910 - val_acc: 0.9509\n",
      "Epoch 140/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0698 - acc: 0.9751 - val_loss: 0.1775 - val_acc: 0.9539\n",
      "Epoch 141/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0740 - acc: 0.9731 - val_loss: 0.1797 - val_acc: 0.9579\n",
      "Epoch 142/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0775 - acc: 0.9716 - val_loss: 0.2568 - val_acc: 0.9288\n",
      "Epoch 143/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0768 - acc: 0.9720 - val_loss: 0.1880 - val_acc: 0.9502\n",
      "Epoch 144/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0857 - acc: 0.9687 - val_loss: 0.1148 - val_acc: 0.9691\n",
      "Epoch 145/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0811 - acc: 0.9708 - val_loss: 0.1464 - val_acc: 0.9644\n",
      "Epoch 146/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0744 - acc: 0.9737 - val_loss: 0.1713 - val_acc: 0.9537\n",
      "Epoch 147/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0727 - acc: 0.9747 - val_loss: 0.1805 - val_acc: 0.9539\n",
      "Epoch 148/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0773 - acc: 0.9742 - val_loss: 0.2091 - val_acc: 0.9462\n",
      "Epoch 149/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0744 - acc: 0.9731 - val_loss: 0.1671 - val_acc: 0.9589\n",
      "True Positive rate is: 0.957\n",
      "True Negative rate is: 0.96\n"
     ]
    }
   ],
   "source": [
    "%run classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Train on 16060 samples, validate on 4016 samples\n",
      "Epoch 1/5000\n",
      "16060/16060 [==============================] - 1s 74us/step - loss: 0.1297 - acc: 0.9442 - val_loss: 0.2166 - val_acc: 0.8974\n",
      "Epoch 2/5000\n",
      "16060/16060 [==============================] - 1s 37us/step - loss: 0.1262 - acc: 0.9453 - val_loss: 0.1737 - val_acc: 0.9390\n",
      "Epoch 3/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.1256 - acc: 0.9469 - val_loss: 0.1810 - val_acc: 0.9363\n",
      "Epoch 4/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.1234 - acc: 0.9487 - val_loss: 0.2189 - val_acc: 0.8997\n",
      "Epoch 5/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1225 - acc: 0.9467 - val_loss: 0.1722 - val_acc: 0.9437\n",
      "Epoch 6/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.1214 - acc: 0.9486 - val_loss: 0.1603 - val_acc: 0.9504\n",
      "Epoch 7/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1201 - acc: 0.9507 - val_loss: 0.2220 - val_acc: 0.9069\n",
      "Epoch 8/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.1201 - acc: 0.9503 - val_loss: 0.2173 - val_acc: 0.9126\n",
      "Epoch 9/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1183 - acc: 0.9514 - val_loss: 0.1752 - val_acc: 0.9432\n",
      "Epoch 10/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.1190 - acc: 0.9512 - val_loss: 0.1615 - val_acc: 0.9512\n",
      "Epoch 11/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1181 - acc: 0.9500 - val_loss: 0.1700 - val_acc: 0.9472\n",
      "Epoch 12/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.1165 - acc: 0.9509 - val_loss: 0.1541 - val_acc: 0.9534\n",
      "Epoch 13/5000\n",
      "16060/16060 [==============================] - 1s 36us/step - loss: 0.1167 - acc: 0.9521 - val_loss: 0.2474 - val_acc: 0.8832\n",
      "Epoch 14/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1155 - acc: 0.9531 - val_loss: 0.1799 - val_acc: 0.9415\n",
      "Epoch 15/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1157 - acc: 0.9522 - val_loss: 0.1686 - val_acc: 0.9455\n",
      "Epoch 16/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.1139 - acc: 0.9527 - val_loss: 0.2184 - val_acc: 0.9166\n",
      "Epoch 17/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1141 - acc: 0.9526 - val_loss: 0.1700 - val_acc: 0.9435\n",
      "Epoch 18/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1137 - acc: 0.9545 - val_loss: 0.1891 - val_acc: 0.9188\n",
      "Epoch 19/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1137 - acc: 0.9519 - val_loss: 0.1486 - val_acc: 0.9559\n",
      "Epoch 20/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1122 - acc: 0.9540 - val_loss: 0.1727 - val_acc: 0.9380\n",
      "Epoch 21/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1133 - acc: 0.9528 - val_loss: 0.1657 - val_acc: 0.9465\n",
      "Epoch 22/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.1113 - acc: 0.9555 - val_loss: 0.1898 - val_acc: 0.9183\n",
      "Epoch 23/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1120 - acc: 0.9550 - val_loss: 0.1567 - val_acc: 0.9504\n",
      "Epoch 24/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1101 - acc: 0.9552 - val_loss: 0.1478 - val_acc: 0.9562\n",
      "Epoch 25/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.1100 - acc: 0.9542 - val_loss: 0.1670 - val_acc: 0.9462\n",
      "Epoch 26/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1109 - acc: 0.9556 - val_loss: 0.1502 - val_acc: 0.9544\n",
      "Epoch 27/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1092 - acc: 0.9554 - val_loss: 0.1818 - val_acc: 0.9360\n",
      "Epoch 28/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1099 - acc: 0.9544 - val_loss: 0.1292 - val_acc: 0.9644\n",
      "Epoch 29/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1089 - acc: 0.9553 - val_loss: 0.1506 - val_acc: 0.9534\n",
      "Epoch 30/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.1078 - acc: 0.9558 - val_loss: 0.1849 - val_acc: 0.9368\n",
      "Epoch 31/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1069 - acc: 0.9570 - val_loss: 0.1821 - val_acc: 0.9320\n",
      "Epoch 32/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1073 - acc: 0.9558 - val_loss: 0.1542 - val_acc: 0.9532\n",
      "Epoch 33/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1061 - acc: 0.9562 - val_loss: 0.2151 - val_acc: 0.9071\n",
      "Epoch 34/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1069 - acc: 0.9568 - val_loss: 0.1530 - val_acc: 0.9534\n",
      "Epoch 35/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1063 - acc: 0.9562 - val_loss: 0.1961 - val_acc: 0.9236\n",
      "Epoch 36/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1064 - acc: 0.9567 - val_loss: 0.1352 - val_acc: 0.9614\n",
      "Epoch 37/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1070 - acc: 0.9567 - val_loss: 0.1933 - val_acc: 0.9228\n",
      "Epoch 38/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1060 - acc: 0.9573 - val_loss: 0.1380 - val_acc: 0.9572\n",
      "Epoch 39/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1048 - acc: 0.9575 - val_loss: 0.1707 - val_acc: 0.9432\n",
      "Epoch 40/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1042 - acc: 0.9586 - val_loss: 0.1769 - val_acc: 0.9437\n",
      "Epoch 41/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1043 - acc: 0.9578 - val_loss: 0.1473 - val_acc: 0.9564\n",
      "Epoch 42/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1041 - acc: 0.9575 - val_loss: 0.1923 - val_acc: 0.9171\n",
      "Epoch 43/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1028 - acc: 0.9585 - val_loss: 0.1695 - val_acc: 0.9480\n",
      "Epoch 44/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.1022 - acc: 0.9582 - val_loss: 0.1337 - val_acc: 0.9626\n",
      "Epoch 45/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1028 - acc: 0.9587 - val_loss: 0.1677 - val_acc: 0.9437\n",
      "Epoch 46/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1020 - acc: 0.9593 - val_loss: 0.1791 - val_acc: 0.9385\n",
      "Epoch 47/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.1021 - acc: 0.9587 - val_loss: 0.1427 - val_acc: 0.9589\n",
      "Epoch 48/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1021 - acc: 0.9591 - val_loss: 0.1659 - val_acc: 0.9485\n",
      "Epoch 49/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1011 - acc: 0.9613 - val_loss: 0.1711 - val_acc: 0.9477\n",
      "Epoch 50/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0998 - acc: 0.9605 - val_loss: 0.1423 - val_acc: 0.9579\n",
      "Epoch 51/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1015 - acc: 0.9585 - val_loss: 0.1966 - val_acc: 0.9243\n",
      "Epoch 52/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1004 - acc: 0.9602 - val_loss: 0.1662 - val_acc: 0.9387\n",
      "Epoch 53/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1017 - acc: 0.9588 - val_loss: 0.1743 - val_acc: 0.9432\n",
      "Epoch 54/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.1005 - acc: 0.9598 - val_loss: 0.2054 - val_acc: 0.9056\n",
      "Epoch 55/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.1004 - acc: 0.9592 - val_loss: 0.1475 - val_acc: 0.9544\n",
      "Epoch 56/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0992 - acc: 0.9609 - val_loss: 0.1653 - val_acc: 0.9470\n",
      "Epoch 57/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.1001 - acc: 0.9595 - val_loss: 0.1487 - val_acc: 0.9564\n",
      "Epoch 58/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0990 - acc: 0.9601 - val_loss: 0.1803 - val_acc: 0.9407\n",
      "Epoch 59/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0989 - acc: 0.9618 - val_loss: 0.1374 - val_acc: 0.9614\n",
      "Epoch 60/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0979 - acc: 0.9613 - val_loss: 0.1546 - val_acc: 0.9537\n",
      "Epoch 61/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0992 - acc: 0.9616 - val_loss: 0.1607 - val_acc: 0.9522\n",
      "Epoch 62/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0968 - acc: 0.9618 - val_loss: 0.1704 - val_acc: 0.9472\n",
      "Epoch 63/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0984 - acc: 0.9619 - val_loss: 0.1549 - val_acc: 0.9537\n",
      "Epoch 64/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0972 - acc: 0.9625 - val_loss: 0.2056 - val_acc: 0.9223\n",
      "Epoch 65/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0990 - acc: 0.9607 - val_loss: 0.1793 - val_acc: 0.9233\n",
      "Epoch 66/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0970 - acc: 0.9613 - val_loss: 0.1344 - val_acc: 0.9644\n",
      "Epoch 67/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0972 - acc: 0.9616 - val_loss: 0.1591 - val_acc: 0.9524\n",
      "Epoch 68/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0956 - acc: 0.9630 - val_loss: 0.1194 - val_acc: 0.9696\n",
      "Epoch 69/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0955 - acc: 0.9623 - val_loss: 0.2253 - val_acc: 0.9011\n",
      "Epoch 70/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0955 - acc: 0.9636 - val_loss: 0.1532 - val_acc: 0.9542\n",
      "Epoch 71/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0960 - acc: 0.9636 - val_loss: 0.1425 - val_acc: 0.9589\n",
      "Epoch 72/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0960 - acc: 0.9610 - val_loss: 0.1322 - val_acc: 0.9639\n",
      "Epoch 73/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0956 - acc: 0.9618 - val_loss: 0.1750 - val_acc: 0.9380\n",
      "Epoch 74/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0942 - acc: 0.9644 - val_loss: 0.1353 - val_acc: 0.9631\n",
      "Epoch 75/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0951 - acc: 0.9646 - val_loss: 0.1495 - val_acc: 0.9562\n",
      "Epoch 76/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0940 - acc: 0.9644 - val_loss: 0.1734 - val_acc: 0.9447\n",
      "Epoch 77/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0948 - acc: 0.9641 - val_loss: 0.1679 - val_acc: 0.9447\n",
      "Epoch 78/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0940 - acc: 0.9645 - val_loss: 0.1788 - val_acc: 0.9345\n",
      "Epoch 79/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0933 - acc: 0.9644 - val_loss: 0.1465 - val_acc: 0.9579\n",
      "Epoch 80/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0935 - acc: 0.9639 - val_loss: 0.1696 - val_acc: 0.9497\n",
      "Epoch 81/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0930 - acc: 0.9645 - val_loss: 0.1353 - val_acc: 0.9624\n",
      "Epoch 82/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0930 - acc: 0.9648 - val_loss: 0.1483 - val_acc: 0.9579\n",
      "Epoch 83/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0922 - acc: 0.9642 - val_loss: 0.1542 - val_acc: 0.9517\n",
      "Epoch 84/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0936 - acc: 0.9631 - val_loss: 0.1524 - val_acc: 0.9537\n",
      "Epoch 85/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0921 - acc: 0.9648 - val_loss: 0.1511 - val_acc: 0.9567\n",
      "Epoch 86/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0911 - acc: 0.9666 - val_loss: 0.1404 - val_acc: 0.9592\n",
      "Epoch 87/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0916 - acc: 0.9659 - val_loss: 0.1207 - val_acc: 0.9694\n",
      "Epoch 88/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0932 - acc: 0.9638 - val_loss: 0.2116 - val_acc: 0.9208\n",
      "Epoch 89/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0918 - acc: 0.9648 - val_loss: 0.1708 - val_acc: 0.9480\n",
      "Epoch 90/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0904 - acc: 0.9652 - val_loss: 0.1238 - val_acc: 0.9674\n",
      "Epoch 91/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0909 - acc: 0.9656 - val_loss: 0.1634 - val_acc: 0.9492\n",
      "Epoch 92/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0950 - acc: 0.9631 - val_loss: 0.1280 - val_acc: 0.9659\n",
      "Epoch 93/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0910 - acc: 0.9658 - val_loss: 0.1683 - val_acc: 0.9504\n",
      "Epoch 94/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0895 - acc: 0.9669 - val_loss: 0.1367 - val_acc: 0.9614\n",
      "Epoch 95/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0899 - acc: 0.9664 - val_loss: 0.1748 - val_acc: 0.9355\n",
      "Epoch 96/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0885 - acc: 0.9677 - val_loss: 0.1201 - val_acc: 0.9664\n",
      "Epoch 97/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0896 - acc: 0.9666 - val_loss: 0.1611 - val_acc: 0.9517\n",
      "Epoch 98/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0892 - acc: 0.9669 - val_loss: 0.1417 - val_acc: 0.9597\n",
      "Epoch 99/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0888 - acc: 0.9678 - val_loss: 0.1290 - val_acc: 0.9639\n",
      "Epoch 100/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0891 - acc: 0.9662 - val_loss: 0.1238 - val_acc: 0.9639\n",
      "Epoch 101/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0893 - acc: 0.9661 - val_loss: 0.1832 - val_acc: 0.9417\n",
      "Epoch 102/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0886 - acc: 0.9660 - val_loss: 0.1775 - val_acc: 0.9445\n",
      "Epoch 103/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0883 - acc: 0.9664 - val_loss: 0.1433 - val_acc: 0.9572\n",
      "Epoch 104/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0887 - acc: 0.9655 - val_loss: 0.1733 - val_acc: 0.9455\n",
      "Epoch 105/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0876 - acc: 0.9672 - val_loss: 0.1322 - val_acc: 0.9626\n",
      "Epoch 106/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0877 - acc: 0.9664 - val_loss: 0.1350 - val_acc: 0.9612\n",
      "Epoch 107/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0880 - acc: 0.9669 - val_loss: 0.1722 - val_acc: 0.9442\n",
      "Epoch 108/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0886 - acc: 0.9654 - val_loss: 0.1407 - val_acc: 0.9579\n",
      "Epoch 109/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0870 - acc: 0.9679 - val_loss: 0.2104 - val_acc: 0.9265\n",
      "Epoch 110/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0882 - acc: 0.9669 - val_loss: 0.1233 - val_acc: 0.9666\n",
      "Epoch 111/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0857 - acc: 0.9683 - val_loss: 0.1496 - val_acc: 0.9549\n",
      "Epoch 112/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0872 - acc: 0.9660 - val_loss: 0.1739 - val_acc: 0.9407\n",
      "Epoch 113/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0863 - acc: 0.9686 - val_loss: 0.2105 - val_acc: 0.9241\n",
      "Epoch 114/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0877 - acc: 0.9673 - val_loss: 0.1188 - val_acc: 0.9666\n",
      "Epoch 115/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0854 - acc: 0.9684 - val_loss: 0.1620 - val_acc: 0.9500\n",
      "Epoch 116/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0857 - acc: 0.9672 - val_loss: 0.2040 - val_acc: 0.9335\n",
      "Epoch 117/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0854 - acc: 0.9678 - val_loss: 0.1575 - val_acc: 0.9532\n",
      "Epoch 118/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0862 - acc: 0.9675 - val_loss: 0.1344 - val_acc: 0.9631\n",
      "Epoch 119/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0863 - acc: 0.9672 - val_loss: 0.1343 - val_acc: 0.9607\n",
      "Epoch 120/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0842 - acc: 0.9690 - val_loss: 0.1925 - val_acc: 0.9437\n",
      "Epoch 121/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0853 - acc: 0.9676 - val_loss: 0.1603 - val_acc: 0.9514\n",
      "Epoch 122/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0860 - acc: 0.9687 - val_loss: 0.1194 - val_acc: 0.9664\n",
      "Epoch 123/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0838 - acc: 0.9691 - val_loss: 0.1326 - val_acc: 0.9631\n",
      "Epoch 124/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0862 - acc: 0.9678 - val_loss: 0.1465 - val_acc: 0.9572\n",
      "Epoch 125/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0845 - acc: 0.9683 - val_loss: 0.1131 - val_acc: 0.9716\n",
      "Epoch 126/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0849 - acc: 0.9691 - val_loss: 0.1473 - val_acc: 0.9559\n",
      "Epoch 127/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0832 - acc: 0.9686 - val_loss: 0.1447 - val_acc: 0.9577\n",
      "Epoch 128/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0834 - acc: 0.9691 - val_loss: 0.1193 - val_acc: 0.9694\n",
      "Epoch 129/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0854 - acc: 0.9682 - val_loss: 0.2072 - val_acc: 0.9161\n",
      "Epoch 130/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0839 - acc: 0.9690 - val_loss: 0.1798 - val_acc: 0.9430\n",
      "Epoch 131/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0822 - acc: 0.9697 - val_loss: 0.1529 - val_acc: 0.9522\n",
      "Epoch 132/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0820 - acc: 0.9699 - val_loss: 0.1826 - val_acc: 0.9410\n",
      "Epoch 133/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0833 - acc: 0.9692 - val_loss: 0.1458 - val_acc: 0.9587\n",
      "Epoch 134/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0825 - acc: 0.9702 - val_loss: 0.1254 - val_acc: 0.9654\n",
      "Epoch 135/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0826 - acc: 0.9697 - val_loss: 0.1976 - val_acc: 0.9365\n",
      "Epoch 136/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0835 - acc: 0.9682 - val_loss: 0.1327 - val_acc: 0.9622\n",
      "Epoch 137/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0839 - acc: 0.9684 - val_loss: 0.1311 - val_acc: 0.9626\n",
      "Epoch 138/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0825 - acc: 0.9700 - val_loss: 0.2258 - val_acc: 0.9153\n",
      "Epoch 139/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0816 - acc: 0.9705 - val_loss: 0.1605 - val_acc: 0.9509\n",
      "Epoch 140/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0833 - acc: 0.9697 - val_loss: 0.1719 - val_acc: 0.9485\n",
      "Epoch 141/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0803 - acc: 0.9712 - val_loss: 0.1858 - val_acc: 0.9422\n",
      "Epoch 142/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0813 - acc: 0.9699 - val_loss: 0.1325 - val_acc: 0.9594\n",
      "Epoch 143/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0805 - acc: 0.9710 - val_loss: 0.2027 - val_acc: 0.9328\n",
      "Epoch 144/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0823 - acc: 0.9689 - val_loss: 0.1756 - val_acc: 0.9457\n",
      "Epoch 145/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0813 - acc: 0.9697 - val_loss: 0.1178 - val_acc: 0.9654\n",
      "Epoch 146/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0812 - acc: 0.9691 - val_loss: 0.1368 - val_acc: 0.9589\n",
      "Epoch 147/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0797 - acc: 0.9699 - val_loss: 0.1060 - val_acc: 0.9704\n",
      "Epoch 148/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0815 - acc: 0.9707 - val_loss: 0.1390 - val_acc: 0.9559\n",
      "Epoch 149/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0794 - acc: 0.9700 - val_loss: 0.1744 - val_acc: 0.9470\n",
      "Epoch 150/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0818 - acc: 0.9705 - val_loss: 0.1107 - val_acc: 0.9686\n",
      "Epoch 151/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0849 - acc: 0.9682 - val_loss: 0.1230 - val_acc: 0.9641\n",
      "Epoch 152/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0781 - acc: 0.9720 - val_loss: 0.2149 - val_acc: 0.9265\n",
      "Epoch 153/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0802 - acc: 0.9704 - val_loss: 0.1412 - val_acc: 0.9572\n",
      "Epoch 154/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0791 - acc: 0.9711 - val_loss: 0.2106 - val_acc: 0.9330\n",
      "Epoch 155/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0792 - acc: 0.9705 - val_loss: 0.1328 - val_acc: 0.9592\n",
      "Epoch 156/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0801 - acc: 0.9706 - val_loss: 0.1183 - val_acc: 0.9664\n",
      "Epoch 157/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0798 - acc: 0.9707 - val_loss: 0.1541 - val_acc: 0.9534\n",
      "Epoch 158/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0801 - acc: 0.9701 - val_loss: 0.1651 - val_acc: 0.9470\n",
      "Epoch 159/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0789 - acc: 0.9708 - val_loss: 0.1480 - val_acc: 0.9539\n",
      "Epoch 160/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0787 - acc: 0.9704 - val_loss: 0.1417 - val_acc: 0.9582\n",
      "Epoch 161/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0790 - acc: 0.9713 - val_loss: 0.1695 - val_acc: 0.9482\n",
      "Epoch 162/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0781 - acc: 0.9720 - val_loss: 0.1386 - val_acc: 0.9579\n",
      "Epoch 163/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0791 - acc: 0.9705 - val_loss: 0.1147 - val_acc: 0.9676\n",
      "Epoch 164/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0792 - acc: 0.9705 - val_loss: 0.1781 - val_acc: 0.9437\n",
      "Epoch 165/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0782 - acc: 0.9709 - val_loss: 0.1533 - val_acc: 0.9514\n",
      "Epoch 166/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0767 - acc: 0.9714 - val_loss: 0.1557 - val_acc: 0.9485\n",
      "Epoch 167/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0774 - acc: 0.9720 - val_loss: 0.1320 - val_acc: 0.9614\n",
      "Epoch 168/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0770 - acc: 0.9715 - val_loss: 0.1371 - val_acc: 0.9584\n",
      "Epoch 169/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0771 - acc: 0.9714 - val_loss: 0.1323 - val_acc: 0.9584\n",
      "Epoch 170/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0786 - acc: 0.9704 - val_loss: 0.1328 - val_acc: 0.9594\n",
      "Epoch 171/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0767 - acc: 0.9724 - val_loss: 0.1484 - val_acc: 0.9572\n",
      "Epoch 172/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0770 - acc: 0.9720 - val_loss: 0.1521 - val_acc: 0.9514\n",
      "Epoch 173/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0769 - acc: 0.9719 - val_loss: 0.1822 - val_acc: 0.9450\n",
      "Epoch 174/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0779 - acc: 0.9714 - val_loss: 0.1343 - val_acc: 0.9597\n",
      "Epoch 175/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0764 - acc: 0.9714 - val_loss: 0.1712 - val_acc: 0.9480\n",
      "Epoch 176/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0776 - acc: 0.9720 - val_loss: 0.1587 - val_acc: 0.9522\n",
      "Epoch 177/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0783 - acc: 0.9710 - val_loss: 0.1983 - val_acc: 0.9353\n",
      "Epoch 178/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0799 - acc: 0.9703 - val_loss: 0.1360 - val_acc: 0.9582\n",
      "Epoch 179/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0773 - acc: 0.9716 - val_loss: 0.1648 - val_acc: 0.9482\n",
      "Epoch 180/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0762 - acc: 0.9725 - val_loss: 0.1782 - val_acc: 0.9447\n",
      "Epoch 181/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0747 - acc: 0.9725 - val_loss: 0.1337 - val_acc: 0.9594\n",
      "Epoch 182/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0772 - acc: 0.9715 - val_loss: 0.1277 - val_acc: 0.9622\n",
      "Epoch 183/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0759 - acc: 0.9710 - val_loss: 0.1527 - val_acc: 0.9527\n",
      "Epoch 184/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0775 - acc: 0.9726 - val_loss: 0.1664 - val_acc: 0.9495\n",
      "Epoch 185/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0764 - acc: 0.9722 - val_loss: 0.1817 - val_acc: 0.9435\n",
      "Epoch 186/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0752 - acc: 0.9717 - val_loss: 0.1448 - val_acc: 0.9569\n",
      "Epoch 187/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0753 - acc: 0.9729 - val_loss: 0.1568 - val_acc: 0.9487\n",
      "Epoch 188/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0741 - acc: 0.9732 - val_loss: 0.1171 - val_acc: 0.9669\n",
      "Epoch 189/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0778 - acc: 0.9729 - val_loss: 0.1378 - val_acc: 0.9589\n",
      "Epoch 190/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0743 - acc: 0.9724 - val_loss: 0.1308 - val_acc: 0.9614\n",
      "Epoch 191/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0754 - acc: 0.9719 - val_loss: 0.1826 - val_acc: 0.9445\n",
      "Epoch 192/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0771 - acc: 0.9718 - val_loss: 0.1231 - val_acc: 0.9626\n",
      "Epoch 193/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0770 - acc: 0.9722 - val_loss: 0.1733 - val_acc: 0.9445\n",
      "Epoch 194/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0745 - acc: 0.9724 - val_loss: 0.1687 - val_acc: 0.9482\n",
      "Epoch 195/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0759 - acc: 0.9724 - val_loss: 0.1284 - val_acc: 0.9617\n",
      "Epoch 196/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0765 - acc: 0.9717 - val_loss: 0.1479 - val_acc: 0.9549\n",
      "Epoch 197/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0746 - acc: 0.9737 - val_loss: 0.1214 - val_acc: 0.9634\n",
      "True Positive rate is: 0.958\n",
      "True Negative rate is: 0.962\n"
     ]
    }
   ],
   "source": [
    "%run classifier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，分类器无论是对攻击数据（True Positive rate）还是正常数据（True Negative rate）的识别率都已经达到了很高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但训练仍然存在数据不平衡的问题。（主要是因为训练数据中的攻击数据依然远多于实际情况）这个问题可以使用分类器的输出阈值来调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16060 samples, validate on 4016 samples\n",
      "Epoch 1/5000\n",
      "16060/16060 [==============================] - 1s 61us/step - loss: 0.0814 - acc: 0.9699 - val_loss: 0.1736 - val_acc: 0.9437\n",
      "Epoch 2/5000\n",
      "16060/16060 [==============================] - 1s 38us/step - loss: 0.0810 - acc: 0.9704 - val_loss: 0.2170 - val_acc: 0.9328\n",
      "Epoch 3/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0805 - acc: 0.9691 - val_loss: 0.1464 - val_acc: 0.9579\n",
      "Epoch 4/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0803 - acc: 0.9707 - val_loss: 0.1177 - val_acc: 0.9674\n",
      "Epoch 5/5000\n",
      "16060/16060 [==============================] - 1s 37us/step - loss: 0.0807 - acc: 0.9692 - val_loss: 0.1785 - val_acc: 0.9455\n",
      "Epoch 6/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0804 - acc: 0.9696 - val_loss: 0.1268 - val_acc: 0.9612\n",
      "Epoch 7/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0794 - acc: 0.9707 - val_loss: 0.1512 - val_acc: 0.9529\n",
      "Epoch 8/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0796 - acc: 0.9710 - val_loss: 0.1847 - val_acc: 0.9427\n",
      "Epoch 9/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0806 - acc: 0.9702 - val_loss: 0.1660 - val_acc: 0.9475\n",
      "Epoch 10/5000\n",
      "16060/16060 [==============================] - 1s 36us/step - loss: 0.0798 - acc: 0.9705 - val_loss: 0.1660 - val_acc: 0.9482\n",
      "Epoch 11/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0789 - acc: 0.9716 - val_loss: 0.1450 - val_acc: 0.9592\n",
      "Epoch 12/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0783 - acc: 0.9715 - val_loss: 0.1476 - val_acc: 0.9559\n",
      "Epoch 13/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0791 - acc: 0.9711 - val_loss: 0.1253 - val_acc: 0.9626\n",
      "Epoch 14/5000\n",
      "16060/16060 [==============================] - 1s 35us/step - loss: 0.0781 - acc: 0.9717 - val_loss: 0.1208 - val_acc: 0.9674\n",
      "Epoch 15/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0789 - acc: 0.9700 - val_loss: 0.1547 - val_acc: 0.9529\n",
      "Epoch 16/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0780 - acc: 0.9704 - val_loss: 0.1035 - val_acc: 0.9719\n",
      "Epoch 17/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0785 - acc: 0.9709 - val_loss: 0.1461 - val_acc: 0.9534\n",
      "Epoch 18/5000\n",
      "16060/16060 [==============================] - 1s 36us/step - loss: 0.0774 - acc: 0.9714 - val_loss: 0.1727 - val_acc: 0.9472\n",
      "Epoch 19/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0803 - acc: 0.9697 - val_loss: 0.2291 - val_acc: 0.9203\n",
      "Epoch 20/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0819 - acc: 0.9684 - val_loss: 0.1715 - val_acc: 0.9465\n",
      "Epoch 21/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0772 - acc: 0.9719 - val_loss: 0.1430 - val_acc: 0.9589\n",
      "Epoch 22/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0781 - acc: 0.9712 - val_loss: 0.1792 - val_acc: 0.9452\n",
      "Epoch 23/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0766 - acc: 0.9725 - val_loss: 0.1185 - val_acc: 0.9654\n",
      "Epoch 24/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0776 - acc: 0.9707 - val_loss: 0.1345 - val_acc: 0.9587\n",
      "Epoch 25/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0804 - acc: 0.9704 - val_loss: 0.1597 - val_acc: 0.9529\n",
      "Epoch 26/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0763 - acc: 0.9710 - val_loss: 0.1249 - val_acc: 0.9602\n",
      "Epoch 27/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0769 - acc: 0.9713 - val_loss: 0.1540 - val_acc: 0.9544\n",
      "Epoch 28/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0782 - acc: 0.9709 - val_loss: 0.1541 - val_acc: 0.9514\n",
      "Epoch 29/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0791 - acc: 0.9693 - val_loss: 0.1314 - val_acc: 0.9589\n",
      "Epoch 30/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0774 - acc: 0.9714 - val_loss: 0.1502 - val_acc: 0.9504\n",
      "Epoch 31/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0761 - acc: 0.9717 - val_loss: 0.1382 - val_acc: 0.9587\n",
      "Epoch 32/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0772 - acc: 0.9712 - val_loss: 0.1477 - val_acc: 0.9552\n",
      "Epoch 33/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0758 - acc: 0.9728 - val_loss: 0.1526 - val_acc: 0.9524\n",
      "Epoch 34/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0766 - acc: 0.9716 - val_loss: 0.1664 - val_acc: 0.9500\n",
      "Epoch 35/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0744 - acc: 0.9727 - val_loss: 0.1556 - val_acc: 0.9537\n",
      "Epoch 36/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0781 - acc: 0.9710 - val_loss: 0.1233 - val_acc: 0.9622\n",
      "Epoch 37/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0748 - acc: 0.9735 - val_loss: 0.1527 - val_acc: 0.9554\n",
      "Epoch 38/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0752 - acc: 0.9729 - val_loss: 0.1975 - val_acc: 0.9390\n",
      "Epoch 39/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0758 - acc: 0.9721 - val_loss: 0.1322 - val_acc: 0.9599\n",
      "Epoch 40/5000\n",
      "16060/16060 [==============================] - ETA: 0s - loss: 0.0760 - acc: 0.971 - 1s 31us/step - loss: 0.0758 - acc: 0.9715 - val_loss: 0.1444 - val_acc: 0.9557\n",
      "Epoch 41/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0749 - acc: 0.9720 - val_loss: 0.1556 - val_acc: 0.9519\n",
      "Epoch 42/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0765 - acc: 0.9718 - val_loss: 0.1484 - val_acc: 0.9522\n",
      "Epoch 43/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0764 - acc: 0.9713 - val_loss: 0.1832 - val_acc: 0.9410\n",
      "Epoch 44/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0752 - acc: 0.9719 - val_loss: 0.1913 - val_acc: 0.9405\n",
      "Epoch 45/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0751 - acc: 0.9720 - val_loss: 0.1652 - val_acc: 0.9410\n",
      "Epoch 46/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0758 - acc: 0.9719 - val_loss: 0.1206 - val_acc: 0.9651\n",
      "Epoch 47/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0757 - acc: 0.9707 - val_loss: 0.1438 - val_acc: 0.9539\n",
      "Epoch 48/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0746 - acc: 0.9725 - val_loss: 0.1224 - val_acc: 0.9639\n",
      "Epoch 49/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0738 - acc: 0.9723 - val_loss: 0.1439 - val_acc: 0.9552\n",
      "Epoch 50/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0740 - acc: 0.9738 - val_loss: 0.0979 - val_acc: 0.9746\n",
      "Epoch 51/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0753 - acc: 0.9706 - val_loss: 0.1612 - val_acc: 0.9527\n",
      "Epoch 52/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0744 - acc: 0.9732 - val_loss: 0.1285 - val_acc: 0.9612\n",
      "Epoch 53/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0744 - acc: 0.9725 - val_loss: 0.1599 - val_acc: 0.9500\n",
      "Epoch 54/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0745 - acc: 0.9724 - val_loss: 0.1624 - val_acc: 0.9504\n",
      "Epoch 55/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0729 - acc: 0.9729 - val_loss: 0.1053 - val_acc: 0.9711\n",
      "Epoch 56/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0730 - acc: 0.9725 - val_loss: 0.1379 - val_acc: 0.9562\n",
      "Epoch 57/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0763 - acc: 0.9715 - val_loss: 0.1283 - val_acc: 0.9617\n",
      "Epoch 58/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0738 - acc: 0.9723 - val_loss: 0.1591 - val_acc: 0.9519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0722 - acc: 0.9737 - val_loss: 0.1649 - val_acc: 0.9500\n",
      "Epoch 60/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0734 - acc: 0.9717 - val_loss: 0.1197 - val_acc: 0.9639\n",
      "Epoch 61/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0744 - acc: 0.9725 - val_loss: 0.1536 - val_acc: 0.9542\n",
      "Epoch 62/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0744 - acc: 0.9729 - val_loss: 0.1641 - val_acc: 0.9472\n",
      "Epoch 63/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0727 - acc: 0.9737 - val_loss: 0.1586 - val_acc: 0.9527\n",
      "Epoch 64/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0719 - acc: 0.9733 - val_loss: 0.1640 - val_acc: 0.9507\n",
      "Epoch 65/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0748 - acc: 0.9722 - val_loss: 0.2090 - val_acc: 0.9318\n",
      "Epoch 66/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0730 - acc: 0.9733 - val_loss: 0.2388 - val_acc: 0.9218\n",
      "Epoch 67/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0731 - acc: 0.9732 - val_loss: 0.1467 - val_acc: 0.9574\n",
      "Epoch 68/5000\n",
      "16060/16060 [==============================] - 1s 34us/step - loss: 0.0732 - acc: 0.9730 - val_loss: 0.1650 - val_acc: 0.9524\n",
      "Epoch 69/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0733 - acc: 0.9724 - val_loss: 0.1244 - val_acc: 0.9661\n",
      "Epoch 70/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0735 - acc: 0.9725 - val_loss: 0.1257 - val_acc: 0.9666\n",
      "Epoch 71/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0721 - acc: 0.9736 - val_loss: 0.1144 - val_acc: 0.9666\n",
      "Epoch 72/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0720 - acc: 0.9730 - val_loss: 0.1459 - val_acc: 0.9609\n",
      "Epoch 73/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0711 - acc: 0.9735 - val_loss: 0.1404 - val_acc: 0.9584\n",
      "Epoch 74/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0731 - acc: 0.9725 - val_loss: 0.1112 - val_acc: 0.9689\n",
      "Epoch 75/5000\n",
      "16060/16060 [==============================] - 0s 30us/step - loss: 0.0725 - acc: 0.9737 - val_loss: 0.2198 - val_acc: 0.9315\n",
      "Epoch 76/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0752 - acc: 0.9717 - val_loss: 0.1047 - val_acc: 0.9724\n",
      "Epoch 77/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0716 - acc: 0.9737 - val_loss: 0.1678 - val_acc: 0.9470\n",
      "Epoch 78/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0712 - acc: 0.9735 - val_loss: 0.1352 - val_acc: 0.9619\n",
      "Epoch 79/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0713 - acc: 0.9736 - val_loss: 0.1628 - val_acc: 0.9519\n",
      "Epoch 80/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0709 - acc: 0.9746 - val_loss: 0.1713 - val_acc: 0.9490\n",
      "Epoch 81/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0714 - acc: 0.9734 - val_loss: 0.1581 - val_acc: 0.9539\n",
      "Epoch 82/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0712 - acc: 0.9730 - val_loss: 0.1782 - val_acc: 0.9467\n",
      "Epoch 83/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0725 - acc: 0.9725 - val_loss: 0.1731 - val_acc: 0.9507\n",
      "Epoch 84/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0698 - acc: 0.9741 - val_loss: 0.1361 - val_acc: 0.9612\n",
      "Epoch 85/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0711 - acc: 0.9738 - val_loss: 0.1393 - val_acc: 0.9607\n",
      "Epoch 86/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0731 - acc: 0.9718 - val_loss: 0.1766 - val_acc: 0.9492\n",
      "Epoch 87/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0700 - acc: 0.9744 - val_loss: 0.1578 - val_acc: 0.9527\n",
      "Epoch 88/5000\n",
      "16060/16060 [==============================] - 1s 31us/step - loss: 0.0706 - acc: 0.9743 - val_loss: 0.2156 - val_acc: 0.9338\n",
      "Epoch 89/5000\n",
      "16060/16060 [==============================] - 1s 33us/step - loss: 0.0693 - acc: 0.9744 - val_loss: 0.2103 - val_acc: 0.9405\n",
      "Epoch 90/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0710 - acc: 0.9738 - val_loss: 0.1445 - val_acc: 0.9577\n",
      "Epoch 91/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0721 - acc: 0.9730 - val_loss: 0.1415 - val_acc: 0.9592\n",
      "Epoch 92/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0708 - acc: 0.9735 - val_loss: 0.1263 - val_acc: 0.9669\n",
      "Epoch 93/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0712 - acc: 0.9728 - val_loss: 0.1232 - val_acc: 0.9651\n",
      "Epoch 94/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0709 - acc: 0.9739 - val_loss: 0.1418 - val_acc: 0.9572\n",
      "Epoch 95/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0685 - acc: 0.9756 - val_loss: 0.1505 - val_acc: 0.9567\n",
      "Epoch 96/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0697 - acc: 0.9738 - val_loss: 0.1637 - val_acc: 0.9512\n",
      "Epoch 97/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0705 - acc: 0.9740 - val_loss: 0.1938 - val_acc: 0.9427\n",
      "Epoch 98/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0701 - acc: 0.9739 - val_loss: 0.1628 - val_acc: 0.9509\n",
      "Epoch 99/5000\n",
      "16060/16060 [==============================] - 0s 31us/step - loss: 0.0708 - acc: 0.9747 - val_loss: 0.1682 - val_acc: 0.9507\n",
      "Epoch 100/5000\n",
      "16060/16060 [==============================] - 1s 32us/step - loss: 0.0707 - acc: 0.9730 - val_loss: 0.1823 - val_acc: 0.9462\n",
      "True Positive rate is: 0.926\n",
      "True Negative rate is: 0.975\n"
     ]
    }
   ],
   "source": [
    "%run classifier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过设定阈值可以将准确率向一边偏移。这与方法一近乎同理。但分类器显然准确率更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12860 samples, validate on 3216 samples\n",
      "Epoch 1/5000\n",
      "12860/12860 [==============================] - 1s 56us/step - loss: 0.2632 - acc: 0.9442 - val_loss: 0.5613 - val_acc: 0.7491\n",
      "Epoch 2/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1835 - acc: 0.9435 - val_loss: 0.6475 - val_acc: 0.7410\n",
      "Epoch 3/5000\n",
      "12860/12860 [==============================] - 0s 37us/step - loss: 0.1781 - acc: 0.9452 - val_loss: 0.4812 - val_acc: 0.7727\n",
      "Epoch 4/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1751 - acc: 0.9460 - val_loss: 0.5031 - val_acc: 0.7724\n",
      "Epoch 5/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1724 - acc: 0.9466 - val_loss: 0.4838 - val_acc: 0.7842\n",
      "Epoch 6/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1687 - acc: 0.9473 - val_loss: 0.4679 - val_acc: 0.7954\n",
      "Epoch 7/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1689 - acc: 0.9481 - val_loss: 0.5554 - val_acc: 0.7749\n",
      "Epoch 8/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1643 - acc: 0.9492 - val_loss: 0.5529 - val_acc: 0.7873\n",
      "Epoch 9/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1654 - acc: 0.9499 - val_loss: 0.4187 - val_acc: 0.8237\n",
      "Epoch 10/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1622 - acc: 0.9501 - val_loss: 0.4639 - val_acc: 0.8162\n",
      "Epoch 11/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1603 - acc: 0.9514 - val_loss: 0.5162 - val_acc: 0.8022\n",
      "Epoch 12/5000\n",
      "12860/12860 [==============================] - 0s 29us/step - loss: 0.1601 - acc: 0.9509 - val_loss: 0.4701 - val_acc: 0.8225\n",
      "Epoch 13/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1583 - acc: 0.9519 - val_loss: 0.5102 - val_acc: 0.8134\n",
      "Epoch 14/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1573 - acc: 0.9534 - val_loss: 0.5848 - val_acc: 0.7886\n",
      "Epoch 15/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1579 - acc: 0.9512 - val_loss: 0.4459 - val_acc: 0.8265\n",
      "Epoch 16/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1573 - acc: 0.9513 - val_loss: 0.5289 - val_acc: 0.8007\n",
      "Epoch 17/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1549 - acc: 0.9523 - val_loss: 0.4669 - val_acc: 0.8178\n",
      "Epoch 18/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1538 - acc: 0.9530 - val_loss: 0.5740 - val_acc: 0.8047\n",
      "Epoch 19/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1550 - acc: 0.9520 - val_loss: 0.4418 - val_acc: 0.8324\n",
      "Epoch 20/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1527 - acc: 0.9532 - val_loss: 0.4789 - val_acc: 0.8041\n",
      "Epoch 21/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1510 - acc: 0.9535 - val_loss: 0.5523 - val_acc: 0.8010\n",
      "Epoch 22/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1491 - acc: 0.9540 - val_loss: 0.5158 - val_acc: 0.8060\n",
      "Epoch 23/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1490 - acc: 0.9537 - val_loss: 0.3889 - val_acc: 0.8430\n",
      "Epoch 24/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1509 - acc: 0.9523 - val_loss: 0.5179 - val_acc: 0.8237\n",
      "Epoch 25/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1511 - acc: 0.9521 - val_loss: 0.5271 - val_acc: 0.8150\n",
      "Epoch 26/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1482 - acc: 0.9537 - val_loss: 0.4454 - val_acc: 0.8315\n",
      "Epoch 27/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1481 - acc: 0.9544 - val_loss: 0.4901 - val_acc: 0.8308\n",
      "Epoch 28/5000\n",
      "12860/12860 [==============================] - ETA: 0s - loss: 0.1474 - acc: 0.954 - 0s 31us/step - loss: 0.1463 - acc: 0.9545 - val_loss: 0.3980 - val_acc: 0.8442\n",
      "Epoch 29/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1470 - acc: 0.9540 - val_loss: 0.4374 - val_acc: 0.8402\n",
      "Epoch 30/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1449 - acc: 0.9548 - val_loss: 0.5107 - val_acc: 0.8221\n",
      "Epoch 31/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1447 - acc: 0.9545 - val_loss: 0.5393 - val_acc: 0.8001\n",
      "Epoch 32/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1446 - acc: 0.9544 - val_loss: 0.4942 - val_acc: 0.8287\n",
      "Epoch 33/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1433 - acc: 0.9549 - val_loss: 0.4702 - val_acc: 0.8308\n",
      "Epoch 34/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1439 - acc: 0.9546 - val_loss: 0.5124 - val_acc: 0.8178\n",
      "Epoch 35/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1422 - acc: 0.9550 - val_loss: 0.5126 - val_acc: 0.8053\n",
      "Epoch 36/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1416 - acc: 0.9556 - val_loss: 0.4272 - val_acc: 0.8411\n",
      "Epoch 37/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1425 - acc: 0.9563 - val_loss: 0.4878 - val_acc: 0.8358\n",
      "Epoch 38/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1410 - acc: 0.9559 - val_loss: 0.4468 - val_acc: 0.8430\n",
      "Epoch 39/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1416 - acc: 0.9559 - val_loss: 0.5142 - val_acc: 0.8243\n",
      "Epoch 40/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1406 - acc: 0.9557 - val_loss: 0.5243 - val_acc: 0.8072\n",
      "Epoch 41/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1397 - acc: 0.9559 - val_loss: 0.4657 - val_acc: 0.8308\n",
      "Epoch 42/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1390 - acc: 0.9564 - val_loss: 0.3972 - val_acc: 0.8430\n",
      "Epoch 43/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1466 - acc: 0.9540 - val_loss: 0.5569 - val_acc: 0.8206\n",
      "Epoch 44/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1402 - acc: 0.9542 - val_loss: 0.4311 - val_acc: 0.8420\n",
      "Epoch 45/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1392 - acc: 0.9551 - val_loss: 0.3936 - val_acc: 0.8442\n",
      "Epoch 46/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1355 - acc: 0.9569 - val_loss: 0.5031 - val_acc: 0.8246\n",
      "Epoch 47/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1358 - acc: 0.9572 - val_loss: 0.5749 - val_acc: 0.8013\n",
      "Epoch 48/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1358 - acc: 0.9560 - val_loss: 0.4168 - val_acc: 0.8417\n",
      "Epoch 49/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1352 - acc: 0.9559 - val_loss: 0.4431 - val_acc: 0.8324\n",
      "Epoch 50/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1377 - acc: 0.9564 - val_loss: 0.5069 - val_acc: 0.8312\n",
      "Epoch 51/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1339 - acc: 0.9566 - val_loss: 0.4505 - val_acc: 0.8389\n",
      "Epoch 52/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1338 - acc: 0.9572 - val_loss: 0.4119 - val_acc: 0.8392\n",
      "Epoch 53/5000\n",
      "12860/12860 [==============================] - 0s 37us/step - loss: 0.1327 - acc: 0.9568 - val_loss: 0.4423 - val_acc: 0.8330\n",
      "Epoch 54/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.1317 - acc: 0.9573 - val_loss: 0.4973 - val_acc: 0.8178\n",
      "Epoch 55/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1333 - acc: 0.9563 - val_loss: 0.2974 - val_acc: 0.8775\n",
      "Epoch 56/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1334 - acc: 0.9557 - val_loss: 0.4270 - val_acc: 0.8305\n",
      "Epoch 57/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1315 - acc: 0.9568 - val_loss: 0.3796 - val_acc: 0.8445\n",
      "Epoch 58/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1316 - acc: 0.9565 - val_loss: 0.6160 - val_acc: 0.7702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1314 - acc: 0.9564 - val_loss: 0.5156 - val_acc: 0.8259\n",
      "Epoch 60/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1291 - acc: 0.9574 - val_loss: 0.4275 - val_acc: 0.8318\n",
      "Epoch 61/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1287 - acc: 0.9569 - val_loss: 0.3824 - val_acc: 0.8442\n",
      "Epoch 62/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1290 - acc: 0.9576 - val_loss: 0.4549 - val_acc: 0.8231\n",
      "Epoch 63/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1266 - acc: 0.9574 - val_loss: 0.4276 - val_acc: 0.8408\n",
      "Epoch 64/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1297 - acc: 0.9577 - val_loss: 0.3486 - val_acc: 0.8492\n",
      "Epoch 65/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1284 - acc: 0.9571 - val_loss: 0.4000 - val_acc: 0.8287\n",
      "Epoch 66/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.1261 - acc: 0.9575 - val_loss: 0.3915 - val_acc: 0.8467\n",
      "Epoch 67/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.1292 - acc: 0.9573 - val_loss: 0.4327 - val_acc: 0.8209\n",
      "Epoch 68/5000\n",
      "12860/12860 [==============================] - 1s 40us/step - loss: 0.1258 - acc: 0.9572 - val_loss: 0.4252 - val_acc: 0.8343\n",
      "Epoch 69/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1268 - acc: 0.9567 - val_loss: 0.3406 - val_acc: 0.8573\n",
      "Epoch 70/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1262 - acc: 0.9567 - val_loss: 0.4809 - val_acc: 0.8231\n",
      "Epoch 71/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.1220 - acc: 0.9584 - val_loss: 0.3856 - val_acc: 0.8296\n",
      "Epoch 72/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1229 - acc: 0.9579 - val_loss: 0.4211 - val_acc: 0.8290\n",
      "Epoch 73/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1231 - acc: 0.9586 - val_loss: 0.4696 - val_acc: 0.8178\n",
      "Epoch 74/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1231 - acc: 0.9570 - val_loss: 0.3343 - val_acc: 0.8563\n",
      "Epoch 75/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1228 - acc: 0.9576 - val_loss: 0.4344 - val_acc: 0.8259\n",
      "Epoch 76/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1209 - acc: 0.9580 - val_loss: 0.3360 - val_acc: 0.8504\n",
      "Epoch 77/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.1203 - acc: 0.9583 - val_loss: 0.4439 - val_acc: 0.8414\n",
      "Epoch 78/5000\n",
      "12860/12860 [==============================] - 0s 38us/step - loss: 0.1190 - acc: 0.9583 - val_loss: 0.3502 - val_acc: 0.8638\n",
      "Epoch 79/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1194 - acc: 0.9583 - val_loss: 0.3223 - val_acc: 0.8588\n",
      "Epoch 80/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1187 - acc: 0.9584 - val_loss: 0.4285 - val_acc: 0.8352\n",
      "Epoch 81/5000\n",
      "12860/12860 [==============================] - 0s 37us/step - loss: 0.1221 - acc: 0.9558 - val_loss: 0.3423 - val_acc: 0.8467\n",
      "Epoch 82/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1175 - acc: 0.9582 - val_loss: 0.3530 - val_acc: 0.8588\n",
      "Epoch 83/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1179 - acc: 0.9588 - val_loss: 0.3817 - val_acc: 0.8445\n",
      "Epoch 84/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.1172 - acc: 0.9586 - val_loss: 0.4831 - val_acc: 0.7948\n",
      "Epoch 85/5000\n",
      "12860/12860 [==============================] - 0s 38us/step - loss: 0.1178 - acc: 0.9578 - val_loss: 0.3827 - val_acc: 0.8461\n",
      "Epoch 86/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.1184 - acc: 0.9577 - val_loss: 0.5195 - val_acc: 0.7994\n",
      "Epoch 87/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1152 - acc: 0.9582 - val_loss: 0.5043 - val_acc: 0.8060\n",
      "Epoch 88/5000\n",
      "12860/12860 [==============================] - 1s 41us/step - loss: 0.1160 - acc: 0.9586 - val_loss: 0.4661 - val_acc: 0.8001\n",
      "Epoch 89/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1136 - acc: 0.9589 - val_loss: 0.3357 - val_acc: 0.8542\n",
      "Epoch 90/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1128 - acc: 0.9600 - val_loss: 0.3663 - val_acc: 0.8458\n",
      "Epoch 91/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1121 - acc: 0.9600 - val_loss: 0.3512 - val_acc: 0.8389\n",
      "Epoch 92/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1139 - acc: 0.9593 - val_loss: 0.4680 - val_acc: 0.8305\n",
      "Epoch 93/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1130 - acc: 0.9589 - val_loss: 0.3184 - val_acc: 0.8560\n",
      "Epoch 94/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1127 - acc: 0.9589 - val_loss: 0.4544 - val_acc: 0.8078\n",
      "Epoch 95/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1111 - acc: 0.9583 - val_loss: 0.3261 - val_acc: 0.8598\n",
      "Epoch 96/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1108 - acc: 0.9594 - val_loss: 0.3534 - val_acc: 0.8532\n",
      "Epoch 97/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1111 - acc: 0.9601 - val_loss: 0.4076 - val_acc: 0.8299\n",
      "Epoch 98/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1089 - acc: 0.9602 - val_loss: 0.2935 - val_acc: 0.8747\n",
      "Epoch 99/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1099 - acc: 0.9593 - val_loss: 0.2784 - val_acc: 0.8650\n",
      "Epoch 100/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1084 - acc: 0.9597 - val_loss: 0.3598 - val_acc: 0.8389\n",
      "Epoch 101/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1073 - acc: 0.9603 - val_loss: 0.4562 - val_acc: 0.8324\n",
      "Epoch 102/5000\n",
      "12860/12860 [==============================] - 0s 37us/step - loss: 0.1068 - acc: 0.9599 - val_loss: 0.3792 - val_acc: 0.8514\n",
      "Epoch 103/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1073 - acc: 0.9605 - val_loss: 0.4166 - val_acc: 0.8396\n",
      "Epoch 104/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1043 - acc: 0.9611 - val_loss: 0.3388 - val_acc: 0.8439\n",
      "Epoch 105/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1034 - acc: 0.9604 - val_loss: 0.3181 - val_acc: 0.8716\n",
      "Epoch 106/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1062 - acc: 0.9604 - val_loss: 0.4397 - val_acc: 0.8243\n",
      "Epoch 107/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1071 - acc: 0.9587 - val_loss: 0.2638 - val_acc: 0.8949\n",
      "Epoch 108/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.1041 - acc: 0.9605 - val_loss: 0.3785 - val_acc: 0.8399\n",
      "Epoch 109/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1049 - acc: 0.9610 - val_loss: 0.3677 - val_acc: 0.8340\n",
      "Epoch 110/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1053 - acc: 0.9600 - val_loss: 0.2867 - val_acc: 0.8818\n",
      "Epoch 111/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1014 - acc: 0.9599 - val_loss: 0.3115 - val_acc: 0.8632\n",
      "Epoch 112/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.1016 - acc: 0.9617 - val_loss: 0.2788 - val_acc: 0.8893\n",
      "Epoch 113/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.0992 - acc: 0.9611 - val_loss: 0.4217 - val_acc: 0.8371\n",
      "Epoch 114/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1022 - acc: 0.9617 - val_loss: 0.3184 - val_acc: 0.8601\n",
      "Epoch 115/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0995 - acc: 0.9617 - val_loss: 0.3869 - val_acc: 0.8240\n",
      "Epoch 116/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.1013 - acc: 0.9607 - val_loss: 0.3851 - val_acc: 0.8433\n",
      "Epoch 117/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1014 - acc: 0.9615 - val_loss: 0.4013 - val_acc: 0.8221\n",
      "Epoch 118/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.1017 - acc: 0.9622 - val_loss: 0.3758 - val_acc: 0.8333\n",
      "Epoch 119/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0989 - acc: 0.9613 - val_loss: 0.3271 - val_acc: 0.8520\n",
      "Epoch 120/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0982 - acc: 0.9605 - val_loss: 0.3106 - val_acc: 0.8644\n",
      "Epoch 121/5000\n",
      "12860/12860 [==============================] - 0s 35us/step - loss: 0.0980 - acc: 0.9603 - val_loss: 0.3349 - val_acc: 0.8433\n",
      "Epoch 122/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0954 - acc: 0.9608 - val_loss: 0.2427 - val_acc: 0.8946\n",
      "Epoch 123/5000\n",
      "12860/12860 [==============================] - 0s 38us/step - loss: 0.0959 - acc: 0.9610 - val_loss: 0.3005 - val_acc: 0.8567\n",
      "Epoch 124/5000\n",
      "12860/12860 [==============================] - 0s 36us/step - loss: 0.0986 - acc: 0.9611 - val_loss: 0.2750 - val_acc: 0.8613\n",
      "Epoch 125/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0986 - acc: 0.9609 - val_loss: 0.3871 - val_acc: 0.8396\n",
      "Epoch 126/5000\n",
      "12860/12860 [==============================] - 0s 29us/step - loss: 0.0957 - acc: 0.9615 - val_loss: 0.3642 - val_acc: 0.8402\n",
      "Epoch 127/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0938 - acc: 0.9621 - val_loss: 0.2763 - val_acc: 0.8800\n",
      "Epoch 128/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0944 - acc: 0.9615 - val_loss: 0.2651 - val_acc: 0.8682\n",
      "Epoch 129/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0940 - acc: 0.9620 - val_loss: 0.2365 - val_acc: 0.9002\n",
      "Epoch 130/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0937 - acc: 0.9621 - val_loss: 0.4219 - val_acc: 0.8243\n",
      "Epoch 131/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0955 - acc: 0.9607 - val_loss: 0.3810 - val_acc: 0.8535\n",
      "Epoch 132/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0941 - acc: 0.9632 - val_loss: 0.3795 - val_acc: 0.8383\n",
      "Epoch 133/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0926 - acc: 0.9623 - val_loss: 0.3680 - val_acc: 0.8237\n",
      "Epoch 134/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0918 - acc: 0.9621 - val_loss: 0.2601 - val_acc: 0.8834\n",
      "Epoch 135/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0938 - acc: 0.9624 - val_loss: 0.3013 - val_acc: 0.8738\n",
      "Epoch 136/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0925 - acc: 0.9630 - val_loss: 0.3220 - val_acc: 0.8604\n",
      "Epoch 137/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0915 - acc: 0.9631 - val_loss: 0.2884 - val_acc: 0.8703\n",
      "Epoch 138/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0922 - acc: 0.9628 - val_loss: 0.2835 - val_acc: 0.8632\n",
      "Epoch 139/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0895 - acc: 0.9624 - val_loss: 0.2280 - val_acc: 0.9086\n",
      "Epoch 140/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0907 - acc: 0.9644 - val_loss: 0.3021 - val_acc: 0.8582\n",
      "Epoch 141/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0900 - acc: 0.9625 - val_loss: 0.3412 - val_acc: 0.8635\n",
      "Epoch 142/5000\n",
      "12860/12860 [==============================] - 1s 39us/step - loss: 0.0908 - acc: 0.9631 - val_loss: 0.3252 - val_acc: 0.8374\n",
      "Epoch 143/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0901 - acc: 0.9631 - val_loss: 0.2744 - val_acc: 0.8815\n",
      "Epoch 144/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0917 - acc: 0.9643 - val_loss: 0.4093 - val_acc: 0.8355\n",
      "Epoch 145/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0884 - acc: 0.9620 - val_loss: 0.3249 - val_acc: 0.8392\n",
      "Epoch 146/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0885 - acc: 0.9628 - val_loss: 0.3362 - val_acc: 0.8495\n",
      "Epoch 147/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0895 - acc: 0.9637 - val_loss: 0.3520 - val_acc: 0.8389\n",
      "Epoch 148/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0875 - acc: 0.9634 - val_loss: 0.2897 - val_acc: 0.8800\n",
      "Epoch 149/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0893 - acc: 0.9636 - val_loss: 0.3253 - val_acc: 0.8433\n",
      "Epoch 150/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0892 - acc: 0.9635 - val_loss: 0.3982 - val_acc: 0.8349\n",
      "Epoch 151/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0877 - acc: 0.9640 - val_loss: 0.2707 - val_acc: 0.8831\n",
      "Epoch 152/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0878 - acc: 0.9645 - val_loss: 0.2792 - val_acc: 0.8843\n",
      "Epoch 153/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0864 - acc: 0.9637 - val_loss: 0.2940 - val_acc: 0.8809\n",
      "Epoch 154/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0877 - acc: 0.9652 - val_loss: 0.3656 - val_acc: 0.8675\n",
      "Epoch 155/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0938 - acc: 0.9624 - val_loss: 0.2916 - val_acc: 0.8831\n",
      "Epoch 156/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0867 - acc: 0.9656 - val_loss: 0.2552 - val_acc: 0.8924\n",
      "Epoch 157/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0911 - acc: 0.9626 - val_loss: 0.2777 - val_acc: 0.8775\n",
      "Epoch 158/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0855 - acc: 0.9638 - val_loss: 0.3180 - val_acc: 0.8591\n",
      "Epoch 159/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0850 - acc: 0.9659 - val_loss: 0.2883 - val_acc: 0.8744\n",
      "Epoch 160/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0862 - acc: 0.9656 - val_loss: 0.3606 - val_acc: 0.8579\n",
      "Epoch 161/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0852 - acc: 0.9644 - val_loss: 0.2215 - val_acc: 0.9269\n",
      "Epoch 162/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0848 - acc: 0.9664 - val_loss: 0.3353 - val_acc: 0.8635\n",
      "Epoch 163/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0844 - acc: 0.9652 - val_loss: 0.3151 - val_acc: 0.8675\n",
      "Epoch 164/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0854 - acc: 0.9651 - val_loss: 0.3465 - val_acc: 0.8613\n",
      "Epoch 165/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0868 - acc: 0.9643 - val_loss: 0.2868 - val_acc: 0.8716\n",
      "Epoch 166/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0857 - acc: 0.9636 - val_loss: 0.2907 - val_acc: 0.8794\n",
      "Epoch 167/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0868 - acc: 0.9654 - val_loss: 0.2577 - val_acc: 0.8952\n",
      "Epoch 168/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0872 - acc: 0.9652 - val_loss: 0.3485 - val_acc: 0.8598\n",
      "Epoch 169/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0841 - acc: 0.9656 - val_loss: 0.1899 - val_acc: 0.9462\n",
      "Epoch 170/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0864 - acc: 0.9658 - val_loss: 0.3577 - val_acc: 0.8588\n",
      "Epoch 171/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0839 - acc: 0.9657 - val_loss: 0.3691 - val_acc: 0.8595\n",
      "Epoch 172/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0857 - acc: 0.9653 - val_loss: 0.2738 - val_acc: 0.8955\n",
      "Epoch 173/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0829 - acc: 0.9668 - val_loss: 0.2803 - val_acc: 0.8868\n",
      "Epoch 174/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0849 - acc: 0.9664 - val_loss: 0.3544 - val_acc: 0.8526\n",
      "Epoch 175/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0866 - acc: 0.9648 - val_loss: 0.3099 - val_acc: 0.8616\n",
      "Epoch 176/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0833 - acc: 0.9663 - val_loss: 0.2592 - val_acc: 0.8999\n",
      "Epoch 177/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0830 - acc: 0.9669 - val_loss: 0.2693 - val_acc: 0.8915\n",
      "Epoch 178/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0836 - acc: 0.9652 - val_loss: 0.3218 - val_acc: 0.8703\n",
      "Epoch 179/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0836 - acc: 0.9662 - val_loss: 0.4322 - val_acc: 0.8433\n",
      "Epoch 180/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0830 - acc: 0.9663 - val_loss: 0.2537 - val_acc: 0.8946\n",
      "Epoch 181/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0797 - acc: 0.9673 - val_loss: 0.2647 - val_acc: 0.8955\n",
      "Epoch 182/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0815 - acc: 0.9667 - val_loss: 0.2804 - val_acc: 0.8905\n",
      "Epoch 183/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0826 - acc: 0.9662 - val_loss: 0.3408 - val_acc: 0.8492\n",
      "Epoch 184/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0875 - acc: 0.9656 - val_loss: 0.2732 - val_acc: 0.8877\n",
      "Epoch 185/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0823 - acc: 0.9665 - val_loss: 0.2584 - val_acc: 0.9111\n",
      "Epoch 186/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0820 - acc: 0.9666 - val_loss: 0.2812 - val_acc: 0.8949\n",
      "Epoch 187/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0798 - acc: 0.9671 - val_loss: 0.2594 - val_acc: 0.8999\n",
      "Epoch 188/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0837 - acc: 0.9654 - val_loss: 0.4321 - val_acc: 0.8150\n",
      "Epoch 189/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0801 - acc: 0.9665 - val_loss: 0.3003 - val_acc: 0.8775\n",
      "Epoch 190/5000\n",
      "12860/12860 [==============================] - 0s 34us/step - loss: 0.0813 - acc: 0.9660 - val_loss: 0.2689 - val_acc: 0.8918\n",
      "Epoch 191/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0813 - acc: 0.9671 - val_loss: 0.2654 - val_acc: 0.8909\n",
      "Epoch 192/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0834 - acc: 0.9659 - val_loss: 0.1944 - val_acc: 0.9356\n",
      "Epoch 193/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0858 - acc: 0.9655 - val_loss: 0.2679 - val_acc: 0.8965\n",
      "Epoch 194/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0816 - acc: 0.9661 - val_loss: 0.2765 - val_acc: 0.8971\n",
      "Epoch 195/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0828 - acc: 0.9667 - val_loss: 0.2229 - val_acc: 0.9182\n",
      "Epoch 196/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0818 - acc: 0.9662 - val_loss: 0.3175 - val_acc: 0.8738\n",
      "Epoch 197/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0851 - acc: 0.9656 - val_loss: 0.3336 - val_acc: 0.8626\n",
      "Epoch 198/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0849 - acc: 0.9650 - val_loss: 0.2093 - val_acc: 0.9400\n",
      "Epoch 199/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0828 - acc: 0.9656 - val_loss: 0.2479 - val_acc: 0.9073\n",
      "Epoch 200/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0795 - acc: 0.9673 - val_loss: 0.3142 - val_acc: 0.8738\n",
      "Epoch 201/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0796 - acc: 0.9666 - val_loss: 0.2937 - val_acc: 0.8756\n",
      "Epoch 202/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0807 - acc: 0.9663 - val_loss: 0.2464 - val_acc: 0.9080\n",
      "Epoch 203/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0789 - acc: 0.9681 - val_loss: 0.3157 - val_acc: 0.8778\n",
      "Epoch 204/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0808 - acc: 0.9668 - val_loss: 0.1928 - val_acc: 0.9394\n",
      "Epoch 205/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0798 - acc: 0.9674 - val_loss: 0.3577 - val_acc: 0.8442\n",
      "Epoch 206/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0802 - acc: 0.9676 - val_loss: 0.3363 - val_acc: 0.8756\n",
      "Epoch 207/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0780 - acc: 0.9679 - val_loss: 0.3267 - val_acc: 0.8601\n",
      "Epoch 208/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0785 - acc: 0.9683 - val_loss: 0.2801 - val_acc: 0.8899\n",
      "Epoch 209/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0820 - acc: 0.9674 - val_loss: 0.3137 - val_acc: 0.8678\n",
      "Epoch 210/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0802 - acc: 0.9680 - val_loss: 0.2806 - val_acc: 0.8881\n",
      "Epoch 211/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0794 - acc: 0.9670 - val_loss: 0.2657 - val_acc: 0.9008\n",
      "Epoch 212/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0792 - acc: 0.9680 - val_loss: 0.3042 - val_acc: 0.8840\n",
      "Epoch 213/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0792 - acc: 0.9672 - val_loss: 0.3347 - val_acc: 0.8778\n",
      "Epoch 214/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0798 - acc: 0.9670 - val_loss: 0.3083 - val_acc: 0.8775\n",
      "Epoch 215/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0778 - acc: 0.9679 - val_loss: 0.3630 - val_acc: 0.8489\n",
      "Epoch 216/5000\n",
      "12860/12860 [==============================] - 0s 33us/step - loss: 0.0783 - acc: 0.9663 - val_loss: 0.2275 - val_acc: 0.9257\n",
      "Epoch 217/5000\n",
      "12860/12860 [==============================] - 0s 30us/step - loss: 0.0778 - acc: 0.9685 - val_loss: 0.4836 - val_acc: 0.8427\n",
      "Epoch 218/5000\n",
      "12860/12860 [==============================] - 0s 32us/step - loss: 0.0830 - acc: 0.9663 - val_loss: 0.2783 - val_acc: 0.8989\n",
      "Epoch 219/5000\n",
      "12860/12860 [==============================] - 0s 31us/step - loss: 0.0790 - acc: 0.9692 - val_loss: 0.2575 - val_acc: 0.8961\n",
      "True Positive rate is: 0.896\n",
      "True Negative rate is: 0.978\n"
     ]
    }
   ],
   "source": [
    "%run classifier_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集中减少一部分攻击数据的数据量，效果与改变阈值类似，但由于数据实实在在减少了，总体准确率下降。但在实际情况中，正常数据远多于攻击数据，分类器的训练会自然向更高的True Negative rate偏移。而对攻击数据的识别已经达到了国六要求，所以该实验前景还是乐观的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验通过使用Autoencoder对正常数据编码再解码的方法，得到了一个只能还原正常数据的神经网络，从而查找出无法还原的异常数据。该方法在本实验的测试数据上达到了95%以上的准确率。但以国六的要求，模型仍然在误报率上较高，这也是神经网络对于不平衡数据固有的困难。但是本次实验初步验证了云端非监督可迭代模型对异常检测的可行性，且实验中的网络还未进行进一步优化。且作为整个IDPS网络的一部分，还是有其作用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个思路，既是将该网络用作第一道防火墙，将被识别的阳性数据（True Positive + False Positive）收集等待下一步检测。这样大大降低了数据的不平衡（如4%的误报数据与96%的攻击数据可能可以被拉到相似的数量级），使之后更准确的识别成为可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
